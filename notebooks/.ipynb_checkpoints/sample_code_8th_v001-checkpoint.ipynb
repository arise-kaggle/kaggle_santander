{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2de096e-247d-400f-a52e-5380c1331268",
   "metadata": {},
   "source": [
    "# xx.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b75bbeb-8d2f-4e21-b80d-a4a3a96912bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math    \n",
    "import io    \n",
    "\n",
    "# ファイル圧縮用途\n",
    "import gzip    \n",
    "import pickle    \n",
    "import zlib    \n",
    "\n",
    "# データ、配列を扱うための基本ライブラリ\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(2016)\n",
    "transformers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6740b8-2acd-4c52-85c4-5b47c64ddddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データとテストデータを1つのデータに統合するコードです。\n",
    "def clean_data(fi, fo, header, suffix):\n",
    "    \n",
    "    # fi : 訓練／テストデータを読み込む file iterator\n",
    "    # fo : 統合されるデータが writeされるパス\n",
    "    # header : データに header 行を追加するかどうかを決定する boolean\n",
    "    # suffix : 訓練データには 48個の変数があり、テストデータには24個の変数があります。\n",
    "    # suffixで不足するテストデータ 24個分を空白で埋めます。\n",
    "    # csvの最初の行、つまり headerを読み込みます。\n",
    "    head = fi.readline().strip(\"\\n\").split(\",\")\n",
    "    head = [h.strip('\"') for h in head]\n",
    "\n",
    "    # ‘‘nomprov’ 変数の位置を ipに保存します。\n",
    "    for i, h in enumerate(head):\n",
    "        if h == \"nomprov\":\n",
    "            ip = i\n",
    "\n",
    "    # headerが True である場合は、保存ファイルの headerを writeします。\n",
    "    if header:\n",
    "        fo.write(\"%s\\n\" % \",\".join(head))\n",
    "\n",
    "    # nは読み込んだ変数の個数を意味します。(訓練データ：48、テストデータ：24)\n",
    "    n = len(head)\n",
    "    for line in fi:\n",
    "        # ファイルの内容を1行ずつ読み出し、改行(\\n)と ‘,’で分離します。\n",
    "        fields = line.strip(\"\\n\").split(\",\")\n",
    "\n",
    "        # ‘‘nomprov’変数に ‘,’を含むデータが存在します。\n",
    "        # ‘,’で分離されたデータを再び組み合わせます。\n",
    "        if len(fields) > n:\n",
    "            prov = fields[ip] + fields[ip+1]\n",
    "            del fields[ip]\n",
    "            fields[ip] = prov\n",
    "\n",
    "        # データの個数が nと同じかどうかを確認し、ファイルに writeします。\n",
    "        # テストデータの場合、suffixは 24個の空白です。\n",
    "        assert len(fields) == n\n",
    "        fields = [field.strip() for field in fields]\n",
    "        fo.write(\"%s%s\\n\" % (\",\".join(fields), suffix))\n",
    "\n",
    "# 1つのデータとして統合するコードを実行します。まず訓練データを writeし、その次にテストデータを writeします。これ以後1つの dataframeだけを取り扱い、前処理を進めます。\n",
    "with open(\"../data/input/8th.clean.all.csv\", \"w\") as f:\n",
    "    clean_data(open(\"../data/input/train_ver2.csv\"), f, True, \"\")\n",
    "    comma24 = \"\".join([\",\" for i in range(24)])\n",
    "    clean_data(open(\"../data/input/test_ver2.csv\"), f, False, comma24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9057123-09df-444c-981f-78008ed27c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (11,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "tmp_pdf = pd.read_csv('../data/input/8th.clean.all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9cf5c63-513e-4507-b224-980bfd0e702a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14576924, 48)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_pdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cf45ce5-94bc-4603-a4fa-12144995e4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_empleado</th>\n",
       "      <th>pais_residencia</th>\n",
       "      <th>sexo</th>\n",
       "      <th>age</th>\n",
       "      <th>fecha_alta</th>\n",
       "      <th>ind_nuevo</th>\n",
       "      <th>antiguedad</th>\n",
       "      <th>indrel</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1</th>\n",
       "      <th>ind_plan_fin_ult1</th>\n",
       "      <th>ind_pres_fin_ult1</th>\n",
       "      <th>ind_reca_fin_ult1</th>\n",
       "      <th>ind_tjcr_fin_ult1</th>\n",
       "      <th>ind_valo_fin_ult1</th>\n",
       "      <th>ind_viv_fin_ult1</th>\n",
       "      <th>ind_nomina_ult1</th>\n",
       "      <th>ind_nom_pens_ult1</th>\n",
       "      <th>ind_recibo_ult1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13647309</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>F</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13647310</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>1170544</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>H</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2013-08-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13647311</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>1170545</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2013-08-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13647312</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>1170547</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>H</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2013-08-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13647313</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>1170548</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>H</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2013-08-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576919</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>660237</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1999-04-21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576920</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>660238</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2006-11-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576921</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>660240</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2006-11-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576922</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>660243</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2006-11-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576923</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>660248</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>V</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2006-11-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>929615 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          fecha_dato  ncodpers ind_empleado pais_residencia sexo   age  \\\n",
       "13647309  2016-06-28     15889            F              ES    V  56.0   \n",
       "13647310  2016-06-28   1170544            N              ES    H  36.0   \n",
       "13647311  2016-06-28   1170545            N              ES    V  22.0   \n",
       "13647312  2016-06-28   1170547            N              ES    H  22.0   \n",
       "13647313  2016-06-28   1170548            N              ES    H  22.0   \n",
       "...              ...       ...          ...             ...  ...   ...   \n",
       "14576919  2016-06-28    660237            N              ES    V  55.0   \n",
       "14576920  2016-06-28    660238            N              ES    V  30.0   \n",
       "14576921  2016-06-28    660240            N              ES    V  52.0   \n",
       "14576922  2016-06-28    660243            N              ES    V  32.0   \n",
       "14576923  2016-06-28    660248            N              ES    V  92.0   \n",
       "\n",
       "          fecha_alta  ind_nuevo  antiguedad  indrel  ... ind_hip_fin_ult1  \\\n",
       "13647309  1995-01-16        0.0       256.0     1.0  ...              NaN   \n",
       "13647310  2013-08-28        0.0        34.0     1.0  ...              NaN   \n",
       "13647311  2013-08-28        0.0        34.0     1.0  ...              NaN   \n",
       "13647312  2013-08-28        0.0        34.0     1.0  ...              NaN   \n",
       "13647313  2013-08-28        0.0        34.0     1.0  ...              NaN   \n",
       "...              ...        ...         ...     ...  ...              ...   \n",
       "14576919  1999-04-21        0.0       206.0     1.0  ...              NaN   \n",
       "14576920  2006-11-29        0.0       115.0     1.0  ...              NaN   \n",
       "14576921  2006-11-29        0.0       115.0     1.0  ...              NaN   \n",
       "14576922  2006-11-29        0.0       115.0     1.0  ...              NaN   \n",
       "14576923  2006-11-29        0.0       115.0     1.0  ...              NaN   \n",
       "\n",
       "         ind_plan_fin_ult1 ind_pres_fin_ult1 ind_reca_fin_ult1  \\\n",
       "13647309               NaN               NaN               NaN   \n",
       "13647310               NaN               NaN               NaN   \n",
       "13647311               NaN               NaN               NaN   \n",
       "13647312               NaN               NaN               NaN   \n",
       "13647313               NaN               NaN               NaN   \n",
       "...                    ...               ...               ...   \n",
       "14576919               NaN               NaN               NaN   \n",
       "14576920               NaN               NaN               NaN   \n",
       "14576921               NaN               NaN               NaN   \n",
       "14576922               NaN               NaN               NaN   \n",
       "14576923               NaN               NaN               NaN   \n",
       "\n",
       "         ind_tjcr_fin_ult1 ind_valo_fin_ult1 ind_viv_fin_ult1 ind_nomina_ult1  \\\n",
       "13647309               NaN               NaN              NaN             NaN   \n",
       "13647310               NaN               NaN              NaN             NaN   \n",
       "13647311               NaN               NaN              NaN             NaN   \n",
       "13647312               NaN               NaN              NaN             NaN   \n",
       "13647313               NaN               NaN              NaN             NaN   \n",
       "...                    ...               ...              ...             ...   \n",
       "14576919               NaN               NaN              NaN             NaN   \n",
       "14576920               NaN               NaN              NaN             NaN   \n",
       "14576921               NaN               NaN              NaN             NaN   \n",
       "14576922               NaN               NaN              NaN             NaN   \n",
       "14576923               NaN               NaN              NaN             NaN   \n",
       "\n",
       "          ind_nom_pens_ult1  ind_recibo_ult1  \n",
       "13647309                NaN              NaN  \n",
       "13647310                NaN              NaN  \n",
       "13647311                NaN              NaN  \n",
       "13647312                NaN              NaN  \n",
       "13647313                NaN              NaN  \n",
       "...                     ...              ...  \n",
       "14576919                NaN              NaN  \n",
       "14576920                NaN              NaN  \n",
       "14576921                NaN              NaN  \n",
       "14576922                NaN              NaN  \n",
       "14576923                NaN              NaN  \n",
       "\n",
       "[929615 rows x 48 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_pdf[tmp_pdf[\"fecha_dato\"]== '2016-06-28']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df4fa96-ca29-4e68-b812-38f295b35c17",
   "metadata": {},
   "source": [
    "# xx.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b68895-48da-4fb6-b208-3f9219a2c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math    \n",
    "import io    \n",
    "import gc\n",
    "\n",
    "# ファイル圧縮用途\n",
    "import gzip    \n",
    "import pickle    \n",
    "import zlib    \n",
    "\n",
    "# データ、配列を扱うための基本ライブラリ\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# カテゴリ型データを数値型データに変換する前処理ツール\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import engines\n",
    "from utils import *\n",
    "\n",
    "np.random.seed(2016)\n",
    "transformers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569bebf7-245a-4418-8d3d-e53a41d2015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_uniq(series, name):\n",
    "    uniq = np.unique(series, return_counts=True)\n",
    "    print(\"assert_uniq\", name, uniq)\n",
    "\n",
    "def custom_one_hot(df, features, name, names, dtype=np.int8, check=False):\n",
    "    for n, val in names.items():\n",
    "        # 新規変数を “変数名_数字”として指定します。\n",
    "        new_name = \"%s_%s\" % (name, n)\n",
    "        # 既存の変数で該当固有値を持っていれば 1, それ以外は 0の2進変数を生成します。\n",
    "        df[new_name] = df[name].map(lambda x: 1 if x == val else 0).astype(dtype)\n",
    "        features.append(new_name)\n",
    "\n",
    "def label_encode(df, features, name):\n",
    "    # データフレームdfの変数 nameの値をすべて stringに変換します。\n",
    "    df[name] = df[name].astype('str')\n",
    "    # 既に,label_encode した変数である場合, transformer[name]にある LabelEncoder()を再活用します。\n",
    "    if name in transformers:\n",
    "        df[name] = transformers[name].transform(df[name])\n",
    "    # 初めて見る変数である場合,transformerにLabelEncoder()を保存し, .fit_transform() 関数で label encodingを遂行します。\n",
    "    else: # train\n",
    "        transformers[name] = LabelEncoder()\n",
    "        df[name] = transformers[name].fit_transform(df[name])\n",
    "    # label encodingした変数は features リストに追加します。\n",
    "    features.append(name)\n",
    "\n",
    "def encode_top(s, count=100, dtype=np.int8):\n",
    "    # すべての固有値に対する頻度を計算します。\n",
    "    uniqs, freqs = np.unique(s, return_counts=True)\n",
    "    # 頻度 Top 100を抽出します。\n",
    "    top = sorted(zip(uniqs,freqs), key=lambda vk: vk[1], reverse = True)[:count]\n",
    "    # { 既存データ：順位 }をあらわす dict()を生成します。\n",
    "    top_map = {uf[0]: l+1 for uf, l in zip(top, range(len(top)))}\n",
    "    # 高頻度 100個のデータは順位に代替し、それ以外は 0 に代替します。\n",
    "    return s.map(lambda x: top_map.get(x, 0)).astype(dtype)\n",
    "\n",
    "def apply_transforms(train_df):\n",
    "    # 学習に使用する変数を保存する features リストを生成します。\n",
    "    features = []\n",
    "\n",
    "    # 2つの変数を label_encode() します。\n",
    "    label_encode(train_df, features, \"canal_entrada\")\n",
    "    label_encode(train_df, features, \"pais_residencia\")\n",
    "\n",
    "    # ageの欠損値を 0.0に代替し、すべての値を整数に変換します。\n",
    "    train_df[\"age\"] = train_df[\"age\"].fillna(0.0).astype(np.int16)\n",
    "    features.append(\"age\")\n",
    "\n",
    "    # rentaの欠損値を 1.0に代替し、 logをかけて分布を変形します。\n",
    "    train_df[\"renta\"].fillna(1.0, inplace=True)\n",
    "    train_df[\"renta\"] = train_df[\"renta\"].map(math.log)\n",
    "    features.append(\"renta\")\n",
    "\n",
    "    # 高頻度の 100個の順位を抽出します。\n",
    "    train_df[\"renta_top\"] = encode_top(train_df[\"renta\"])\n",
    "    features.append(\"renta_top\")\n",
    "\n",
    "    # 欠損値あるいは負の数である場合は 0に代替し、残りの値は +1.0 をした後、整数に変換します。\n",
    "    train_df[\"antiguedad\"] = train_df[\"antiguedad\"].map(lambda x: 0.0 if x < 0 or math.isnan(x) else x+1.0).astype(np.int16)\n",
    "    features.append(\"antiguedad\")\n",
    "\n",
    "    # 欠損値を 0.0に代替し、整数に変換します。\n",
    "    train_df[\"tipodom\"] = train_df[\"tipodom\"].fillna(0.0).astype(np.int8)\n",
    "    features.append(\"tipodom\")\n",
    "    train_df[\"cod_prov\"] = train_df[\"cod_prov\"].fillna(0.0).astype(np.int8)\n",
    "    features.append(\"cod_prov\")\n",
    "\n",
    "    # fecha_datoから月／年度を抽出し、整数値に変換します。\n",
    "    train_df[\"fecha_dato_month\"] = train_df[\"fecha_dato\"].map(lambda x: int(x.split(\"-\")[1])).astype(np.int8)\n",
    "    features.append(\"fecha_dato_month\")\n",
    "    train_df[\"fecha_dato_year\"] = train_df[\"fecha_dato\"].map(lambda x: float(x.split(\"-\")[0])).astype(np.int16)\n",
    "    features.append(\"fecha_dato_year\")\n",
    "\n",
    "    # 欠損値を 0.0に代替し、fecha_altaから月／年度を抽出して整数値に変換します。\n",
    "    # x.__class__が欠損値の場合 floatを変換するため、欠損値探知用に使用します。\n",
    "    train_df[\"fecha_alta_month\"] = train_df[\"fecha_alta\"].map(lambda x: 0.0 if x.__class__ is float else float(x.split(\"-\")[1])).astype(np.int8)\n",
    "    features.append(\"fecha_alta_month\")\n",
    "    train_df[\"fecha_alta_year\"] = train_df[\"fecha_alta\"].map(lambda x: 0.0 if x.__class__ is float else float(x.split(\"-\")[0])).astype(np.int16)\n",
    "    features.append(\"fecha_alta_year\")\n",
    "\n",
    "    # 日付データを月を基準とした数値型変数に変換します。\n",
    "    train_df[\"fecha_dato_float\"] = train_df[\"fecha_dato\"].map(date_to_float)\n",
    "    train_df[\"fecha_alta_float\"] = train_df[\"fecha_alta\"].map(date_to_float)\n",
    "\n",
    "    # fecha_dato と fecha_altoの月を基準とした数値型変数の差異値を派生変数として生成します。\n",
    "    train_df[\"dato_minus_alta\"] = train_df[\"fecha_dato_float\"] - train_df[\"fecha_alta_float\"]\n",
    "    features.append(\"dato_minus_alta\")\n",
    "\n",
    "    # 日付データを月を基準とした数値型変数に変換します (1 ~ 18 間の値に制限)。\n",
    "    train_df[\"int_date\"] = train_df[\"fecha_dato\"].map(date_to_int).astype(np.int8)\n",
    "\n",
    "    # 独自に開発した one-hot-encodingを遂行します。\n",
    "    custom_one_hot(train_df, features, \"indresi\", {\"n\":\"N\"})\n",
    "    custom_one_hot(train_df, features, \"indext\", {\"s\":\"S\"})\n",
    "    custom_one_hot(train_df, features, \"conyuemp\", {\"n\":\"N\"})\n",
    "    custom_one_hot(train_df, features, \"sexo\", {\"h\":\"H\", \"v\":\"V\"})\n",
    "    custom_one_hot(train_df, features, \"ind_empleado\", {\"a\":\"A\", \"b\":\"B\", \"f\":\"F\", \"n\":\"N\"})\n",
    "    custom_one_hot(train_df, features, \"ind_nuevo\", {\"new\":1})\n",
    "    custom_one_hot(train_df, features, \"segmento\", {\"top\":\"01 - TOP\", \"particulares\":\"02 - PARTICULARES\", \"universitario\":\"03 - UNIVERSITARIO\"})\n",
    "    custom_one_hot(train_df, features, \"indfall\", {\"s\":\"S\"})\n",
    "    custom_one_hot(train_df, features, \"tiprel_1mes\", {\"a\":\"A\", \"i\":\"I\", \"p\":\"P\", \"r\":\"R\"}, check=True)\n",
    "    custom_one_hot(train_df, features, \"indrel\", {\"1\":1, \"99\":99})\n",
    "\n",
    "    # 欠損値を0.0に代替し、その他は +1.0を加え、整数に変換します。\n",
    "    train_df[\"ind_actividad_cliente\"] = train_df[\"ind_actividad_cliente\"].map(lambda x: 0.0 if math.isnan(x) else x+1.0).astype(np.int8)\n",
    "    features.append(\"ind_actividad_cliente\")\n",
    "\n",
    "    # 欠損値を 0.0に代替し、 “P”を 5に代替し、整数に変換します。\n",
    "    train_df[\"indrel_1mes\"] = train_df[\"indrel_1mes\"].map(lambda x: 5.0 if x == \"P\" else x).astype(float).fillna(0.0).astype(np.int8)\n",
    "    features.append(\"indrel_1mes\")\n",
    "    \n",
    "    # データ前処理／特徴量エンジニアリングが1次的に完了したデータフレームtrain_dfと、学習に使用する変数リスト featuresを tuple 形式に変換します。\n",
    "    return train_df, tuple(features)\n",
    "\n",
    "\n",
    "def make_prev_df(train_df, step):\n",
    "    # 新しいデータフレームに ncodpersを追加し、int_dateを stepだけ移動した値を入れます。\n",
    "    prev_df = pd.DataFrame()\n",
    "    prev_df[\"ncodpers\"] = train_df[\"ncodpers\"]\n",
    "    prev_df[\"int_date\"] = train_df[\"int_date\"].map(lambda x: x+step).astype(np.int8)\n",
    "\n",
    "    # “変数名_prev1” 形式の lag 変数を生成します。\n",
    "    prod_features = [\"%s_prev%s\" % (prod, step) for prod in products]\n",
    "    for prod, prev in zip(products, prod_features):\n",
    "        prev_df[prev] = train_df[prod]\n",
    "\n",
    "    return prev_df, tuple(prod_features)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # “データ準備”で統合したデータを読み込みます。\n",
    "    fname = \"../data/input/8th.clean.all.csv\"\n",
    "    # train_df = pd.read_csv(fname, dtype=dtypes)\n",
    "\n",
    "    # あとで消す\n",
    "    # 学習データが多いのでサンプリング\n",
    "    tmp_df = pd.read_csv(fname, dtype=dtypes)\n",
    "    trn = tmp_df[tmp_df[\"fecha_dato\"]!= '2016-06-28'].sample(100000)\n",
    "    tst = tmp_df[tmp_df[\"fecha_dato\"]== '2016-06-28']\n",
    "    train_df = pd.DataFrame()\n",
    "    train_df = pd.concat([trn, tst], axis=0)\n",
    "    del tmp_df, trn, tst\n",
    "    gc.collect()\n",
    "    \n",
    "    # productsは util.pyで定義した 24個の金融商品の名前です。\n",
    "    # 欠損値を 0.0で代替し、整数型に変換します。\n",
    "    for prod in products:\n",
    "        train_df[prod] = train_df[prod].fillna(0.0).astype(np.int8)\n",
    "\n",
    "    # 48個の変数ごとに前処理／特徴量エンジニアリングを適用します。\n",
    "    train_df, features = apply_transforms(train_df)\n",
    "\n",
    "    prev_dfs = []\n",
    "    prod_features = None\n",
    "\n",
    "    use_features = frozenset([1,2])\n",
    "    # 1 ~ 5までの stepに対して make_prev_df()を通して lag-n データを生成します。\n",
    "    for step in range(1,6):\n",
    "        prev1_train_df, prod1_features = make_prev_df(train_df, step)\n",
    "        # 生成した lag データは prev_dfs リストに保存します。\n",
    "        prev_dfs.append(prev1_train_df)\n",
    "        # featuresには lag-1,2だけ追加します。\n",
    "        if step in use_features:\n",
    "            features += prod1_features\n",
    "        # prod_featuresには lag-1の変数名だけを保存します。\n",
    "        if step == 1:\n",
    "            prod_features = prod1_features\n",
    "\n",
    "    return train_df, prev_dfs, features, prod_features\n",
    "\n",
    "\n",
    "def join_with_prev(df, prev_df, how):\n",
    "    # pandas merge 関数を通して join\n",
    "    df = df.merge(prev_df, on=[\"ncodpers\", \"int_date\"], how=how)\n",
    "    # 24個の金融変数を小数型に変換します。\n",
    "    for f in set(prev_df.columns.values.tolist()) - set([\"ncodpers\", \"int_date\"]):\n",
    "        df[f] = df[f].astype(np.float16)\n",
    "    return df\n",
    "\n",
    "def make_data():\n",
    "    train_df, prev_dfs, features, prod_features = load_data()\n",
    "    \n",
    "    for i, prev_df in enumerate(prev_dfs):\n",
    "        with Timer(\"join train with prev%s\" % (i+1)):\n",
    "            how = \"inner\" if i == 0 else \"left\"\n",
    "            train_df = join_with_prev(train_df, prev_df, how=how)\n",
    "\n",
    "    # 24個の金融変数に対して for loopをまわします。\n",
    "    for prod in products:\n",
    "        # [1~3], [1~5], [2~5] の3つの区間に対して標準偏差を求めます。\n",
    "        for begin, end in [(1,3),(1,5),(2,5)]:\n",
    "            prods = [\"%s_prev%s\" % (prod, i) for i in range(begin,end+1)]\n",
    "            mp_df = train_df[prods].values\n",
    "            stdf = \"%s_std_%s_%s\" % (prod,begin,end)\n",
    "\n",
    "            # np.nanstdで標準偏差を求め、featuresに新規派生変数の名前を追加します。\n",
    "            train_df[stdf] = np.nanstd(mp_df, axis=1)\n",
    "            features += (stdf,)\n",
    "\n",
    "        # [2~3], [2~5] の2つの区間に対して最小値／最大値を求めます。\n",
    "        for begin, end in [(2,3),(2,5)]:\n",
    "            prods = [\"%s_prev%s\" % (prod, i) for i in range(begin,end+1)]\n",
    "            mp_df = train_df[prods].values\n",
    "\n",
    "            minf = \"%s_min_%s_%s\"%(prod,begin,end)\n",
    "            train_df[minf] = np.nanmin(mp_df, axis=1).astype(np.int8)\n",
    "\n",
    "            maxf = \"%s_max_%s_%s\"%(prod,begin,end)\n",
    "            train_df[maxf] = np.nanmax(mp_df, axis=1).astype(np.int8)\n",
    "\n",
    "            features += (minf,maxf,)\n",
    "\n",
    "    # 顧客の固有識別番号(ncodpers), 整数で表現された日付(int_date), 実際の日付(fecha_dato), 24個の金融変数(products)と学習に使用するために前処理／特徴量エンジニアリングをした変数(features)が重要な変数です。\n",
    "    leave_columns = [\"ncodpers\", \"int_date\", \"fecha_dato\"] + list(products) + list(features)\n",
    "    # 重複値がないかを確認します。\n",
    "    assert len(leave_columns) == len(set(leave_columns))\n",
    "    # train_dfで主要な変数だけを抽出します。\n",
    "    train_df = train_df[leave_columns]\n",
    "\n",
    "    return train_df, features, prod_features\n",
    "\n",
    "\n",
    "def make_submission(f, Y_test, C):\n",
    "    Y_ret = []\n",
    "    # ファイルの最初の行にheaderを書き込みます。\n",
    "    f.write(\"ncodpers,added_products\\n\".encode('utf-8'))\n",
    "    # 顧客識別番号(C)と、予測結果(Y_test)の for loop\n",
    "    for c, y_test in zip(C, Y_test):\n",
    "        # (確率値、金融変数名、金融変数id)の tupleを求めます。\n",
    "        y_prods = [(y,p,ip) for y,p,ip in zip(y_test, products, range(len(products)))]\n",
    "        # 確率値をもとに、上位7個の結果だけを抽出します。\n",
    "        y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "        # 金融変数idを Y_retに保存します。\n",
    "        Y_ret.append([ip for y,p,ip in y_prods])\n",
    "        y_prods = [p for y,p,ip in y_prods]\n",
    "        # ファイルに “顧客識別番号、7個の金融変数”を書き込みます。\n",
    "        f.write((\"%s,%s\\n\" % (int(c), \" \".join(y_prods))).encode('utf-8'))\n",
    "    # 上位7個の予測値を返します。\n",
    "    return Y_ret\n",
    "\n",
    "\n",
    "def train_predict(all_df, features, prod_features, str_date, cv):\n",
    "    # all_df : 統合データ\n",
    "    # features : 学習に使用する変数\n",
    "    # prod_features : 24個の金融変数\n",
    "    # str_date : 予測結果を算出する日付。 2016-05-28の場合は、訓練データの一部であり正答がわかっているので交差検証を意味し、2016-06-28の場合はKaggleにアップロードするデータを生成します。\n",
    "    # cv : 交差検証を実行するかどうか\n",
    "\n",
    "    # str_dateで予測結果を算出する日付を指定します。\n",
    "    test_date = date_to_int(str_date)\n",
    "    # 訓練データは test_date 以前のすべてのデータを使用します。\n",
    "    train_df = all_df[all_df.int_date < test_date]\n",
    "    # テストデータを統合データから分離します。\n",
    "    test_df = pd.DataFrame(all_df[all_df.int_date == test_date])\n",
    "\n",
    "    # 新規購買顧客だけを訓練データへ抽出します。\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i, prod in enumerate(products):\n",
    "        prev = prod + \"_prev1\"\n",
    "        # 新規購買顧客を prX に保存します。\n",
    "        prX = train_df[(train_df[prod] == 1) & (train_df[prev] == 0)]\n",
    "        # prY には新規購買に対する label 値を保存します。\n",
    "        prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n",
    "        X.append(prX)\n",
    "        Y.append(prY)\n",
    "\n",
    "    XY = pd.concat(X)\n",
    "    Y = np.hstack(Y)\n",
    "    # XY は新規購買データだけを含みます。\n",
    "    XY[\"y\"] = Y\n",
    "\n",
    "    # メモリから変数を削除\n",
    "    del train_df\n",
    "    del all_df\n",
    "\n",
    "    # データ別の加重値を計算するため、新しい変数 (ncodpers + fecha_dato)を生成します。\n",
    "    XY[\"ncodepers_fecha_dato\"] = XY[\"ncodpers\"].astype(str) + XY[\"fecha_dato\"]\n",
    "    uniqs, counts = np.unique(XY[\"ncodepers_fecha_dato\"], return_counts=True)\n",
    "    # ネイピア数(e)を用いて, countが高いデータに低い加重値を与えます。\n",
    "    weights = np.exp(1/counts - 1)\n",
    "\n",
    "    # 加重値を XY データに追加します。\n",
    "    wdf = pd.DataFrame()\n",
    "    wdf[\"ncodepers_fecha_dato\"] = uniqs\n",
    "    wdf[\"counts\"] = counts\n",
    "    wdf[\"weight\"] = weights\n",
    "    XY = XY.merge(wdf, on=\"ncodepers_fecha_dato\")\n",
    "\n",
    "    # 交差検証のため、XYを訓練：検証(8:2)に分離します。\n",
    "    mask = np.random.rand(len(XY)) < 0.8\n",
    "    XY_train = XY[mask]\n",
    "    XY_validate = XY[~mask]\n",
    "\n",
    "    # テストデータで加重値はすべて1です。\n",
    "    test_df[\"weight\"] = np.ones(len(test_df), dtype=np.int8)\n",
    "\n",
    "    # テストデータから“新規購買”の正答値を抽出します。\n",
    "    test_df[\"y\"] = test_df[\"ncodpers\"]\n",
    "    Y_prev = test_df[prod_features].values\n",
    "    for prod in products:\n",
    "        prev = prod + \"_prev1\"\n",
    "        padd = prod + \"_add\"\n",
    "        # 新規購買であるかどうかを求めます。\n",
    "        test_df[padd] = test_df[prod] - test_df[prev]\n",
    "\n",
    "    test_add_mat = test_df[[prod + \"_add\" for prod in products]].values\n",
    "    C = test_df[\"ncodpers\"].values\n",
    "    test_add_list = [list() for i in range(len(C))]\n",
    "    # 評価基準 MAP@7 の計算のため、顧客別新規購買正答値をtest_add_listに記録します。\n",
    "    count = 0\n",
    "    for c in range(len(C)):\n",
    "        for p in range(len(products)):\n",
    "            if test_add_mat[c,p] > 0:\n",
    "                test_add_list[c].append(p)\n",
    "                count += 1\n",
    "    \n",
    "    # 交差検証で、テストデータから分離されたデータが得ることのできる\n",
    "    if cv:\n",
    "        max_map7 = mapk(test_add_list, test_add_list, 7, 0.0)\n",
    "        map7coef = float(len(test_add_list)) / float(sum([int(bool(a)) for a in test_add_list]))\n",
    "        print(\"Max MAP@7\", str_date, max_map7, max_map7*map7coef)\n",
    "\n",
    "    # LightGBM モデル学習の後、予測結果を保存します。\n",
    "    Y_test_lgbm = engines.lightgbm(XY_train, XY_validate, test_df, features, XY_all = XY, restore = (str_date == \"2016-06-28\"))\n",
    "    print('Y_test_lgbm : ',Y_test_lgbm.shape)\n",
    "    # test_add_list_lightgbm = make_submission(io.BytesIO() if cv else gzip.open(\"tmp/%s.lightgbm.csv.gz\" % str_date, \"wb\"), Y_test_lgbm - Y_prev, C)\n",
    "    test_add_list_lightgbm = make_submission(io.BytesIO() if cv else gzip.open(\"%s.lightgbm.csv.gz\" % str_date, \"wb\"), Y_test_lgbm - Y_prev, C)\n",
    "    \n",
    "    \n",
    "    # 交差検証の場合, LightGBM モデルのテストデータ MAP@7 評価基準を出力します。\n",
    "    if cv:\n",
    "        map7lightgbm = mapk(test_add_list, test_add_list_lightgbm, 7, 0.0)\n",
    "        print(\"LightGBMlib MAP@7\", str_date, map7lightgbm, map7lightgbm*map7coef)\n",
    "\n",
    "    # XGBoost モデル学習の後、予測結果を保存します。\n",
    "    Y_test_xgb = engines.xgboost(XY_train, XY_validate, test_df, features, XY_all = XY, restore = (str_date == \"2016-06-28\"))\n",
    "    print('Y_test_xgb : ',Y_test_xgb.shape)\n",
    "    # test_add_list_xgboost = make_submission(io.BytesIO() if cv else gzip.open(\"tmp/%s.xgboost.csv.gz\" % str_date, \"wb\"), Y_test_xgb - Y_prev, C)\n",
    "    test_add_list_xgboost = make_submission(io.BytesIO() if cv else gzip.open(\"%s.xgboost.csv.gz\" % str_date, \"wb\"), Y_test_xgb - Y_prev, C)\n",
    "\n",
    "    # 交差検証の場合, XGBoost モデルのテストデータ MAP@7 評価基準を出力します。\n",
    "    if cv:\n",
    "        map7xgboost = mapk(test_add_list, test_add_list_xgboost, 7, 0.0)\n",
    "        print(\"XGBoost MAP@7\", str_date, map7xgboost, map7xgboost*map7coef)\n",
    "\n",
    "    # 平方した後、平行根を求めるやり方でアンサンブルを遂行します。\n",
    "    Y_test = np.sqrt(np.multiply(Y_test_xgb, Y_test_lgbm))\n",
    "    # アンサンブルの結果を保存し、テストデータに対する MAP@7 を出力します。\n",
    "    # test_add_list_xl = make_submission(io.BytesIO() if cv else gzip.open(\"tmp/%s.xgboost-lightgbm.csv.gz\" % str_date, \"wb\"), Y_test - Y_prev, C)\n",
    "    print('Y_test : ',Y_test.shape)\n",
    "    test_add_list_xl = make_submission(io.BytesIO() if cv else gzip.open(\"%s.xgboost-lightgbm.csv.gz\" % str_date, \"wb\"), Y_test - Y_prev, C)\n",
    "\n",
    "    # 正答値の test_add_listとアンサンブルモデルの予測値を mapk 関数に入れて、評価基準の点数を確認します。\n",
    "    if cv:\n",
    "        map7xl = mapk(test_add_list, test_add_list_xl, 7, 0.0)\n",
    "        print(\"XGBoost+LightGBM MAP@7\", str_date, map7xl, map7xl*map7coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d17a300-141c-448e-9fbd-b34c4953541b",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e15b1a0-7c97-4456-b883-2ae74482f17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# “データ準備”で統合したデータを読み込みます。\n",
    "fname = \"../data/input/8th.clean.all.csv\"\n",
    "# train_df = pd.read_csv(fname, dtype=dtypes)\n",
    "\n",
    "# あとで消す\n",
    "# 学習データが多いのでサンプリング\n",
    "tmp_df = pd.read_csv(fname, dtype=dtypes)\n",
    "trn = tmp_df[tmp_df[\"fecha_dato\"]!= '2016-06-28'].sample(100000)\n",
    "tst = tmp_df[tmp_df[\"fecha_dato\"]== '2016-06-28']\n",
    "train_df = pd.DataFrame()\n",
    "train_df = pd.concat([trn, tst], axis=0)\n",
    "del tmp_df, trn, tst\n",
    "gc.collect()\n",
    "\n",
    "# productsは util.pyで定義した 24個の金融商品の名前です。\n",
    "# 欠損値を 0.0で代替し、整数型に変換します。\n",
    "for prod in products:\n",
    "    train_df[prod] = train_df[prod].fillna(0.0).astype(np.int8)\n",
    "\n",
    "# 48個の変数ごとに前処理／特徴量エンジニアリングを適用します。\n",
    "train_df, features = apply_transforms(train_df)\n",
    "\n",
    "prev_dfs = []\n",
    "prod_features = None\n",
    "\n",
    "use_features = frozenset([1,2])\n",
    "# 1 ~ 5までの stepに対して make_prev_df()を通して lag-n データを生成します。\n",
    "for step in range(1,6):\n",
    "    prev1_train_df, prod1_features = make_prev_df(train_df, step)\n",
    "    # 生成した lag データは prev_dfs リストに保存します。\n",
    "    prev_dfs.append(prev1_train_df)\n",
    "    # featuresには lag-1,2だけ追加します。\n",
    "    if step in use_features:\n",
    "        features += prod1_features\n",
    "    # prod_featuresには lag-1の変数名だけを保存します。\n",
    "    if step == 1:\n",
    "        prod_features = prod1_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "102e5da8-2da9-4537-8b5a-9e259a57d4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1029615, 77)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8059d7f-cfea-4568-ba4d-cc7ec623a6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_empleado</th>\n",
       "      <th>pais_residencia</th>\n",
       "      <th>sexo</th>\n",
       "      <th>age</th>\n",
       "      <th>fecha_alta</th>\n",
       "      <th>ind_nuevo</th>\n",
       "      <th>antiguedad</th>\n",
       "      <th>indrel</th>\n",
       "      <th>...</th>\n",
       "      <th>segmento_top</th>\n",
       "      <th>segmento_particulares</th>\n",
       "      <th>segmento_universitario</th>\n",
       "      <th>indfall_s</th>\n",
       "      <th>tiprel_1mes_a</th>\n",
       "      <th>tiprel_1mes_i</th>\n",
       "      <th>tiprel_1mes_p</th>\n",
       "      <th>tiprel_1mes_r</th>\n",
       "      <th>indrel_1</th>\n",
       "      <th>indrel_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13647309</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>F</td>\n",
       "      <td>36</td>\n",
       "      <td>V</td>\n",
       "      <td>56</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>257</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13647310</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>1170544</td>\n",
       "      <td>N</td>\n",
       "      <td>36</td>\n",
       "      <td>H</td>\n",
       "      <td>36</td>\n",
       "      <td>2013-08-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13647311</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>1170545</td>\n",
       "      <td>N</td>\n",
       "      <td>36</td>\n",
       "      <td>V</td>\n",
       "      <td>22</td>\n",
       "      <td>2013-08-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13647312</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>1170547</td>\n",
       "      <td>N</td>\n",
       "      <td>36</td>\n",
       "      <td>H</td>\n",
       "      <td>22</td>\n",
       "      <td>2013-08-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13647313</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>1170548</td>\n",
       "      <td>N</td>\n",
       "      <td>36</td>\n",
       "      <td>H</td>\n",
       "      <td>22</td>\n",
       "      <td>2013-08-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576919</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>660237</td>\n",
       "      <td>N</td>\n",
       "      <td>36</td>\n",
       "      <td>V</td>\n",
       "      <td>55</td>\n",
       "      <td>1999-04-21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>207</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576920</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>660238</td>\n",
       "      <td>N</td>\n",
       "      <td>36</td>\n",
       "      <td>V</td>\n",
       "      <td>30</td>\n",
       "      <td>2006-11-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576921</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>660240</td>\n",
       "      <td>N</td>\n",
       "      <td>36</td>\n",
       "      <td>V</td>\n",
       "      <td>52</td>\n",
       "      <td>2006-11-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576922</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>660243</td>\n",
       "      <td>N</td>\n",
       "      <td>36</td>\n",
       "      <td>V</td>\n",
       "      <td>32</td>\n",
       "      <td>2006-11-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576923</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>660248</td>\n",
       "      <td>N</td>\n",
       "      <td>36</td>\n",
       "      <td>V</td>\n",
       "      <td>92</td>\n",
       "      <td>2006-11-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>929615 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          fecha_dato  ncodpers ind_empleado  pais_residencia sexo  age  \\\n",
       "13647309  2016-06-28     15889            F               36    V   56   \n",
       "13647310  2016-06-28   1170544            N               36    H   36   \n",
       "13647311  2016-06-28   1170545            N               36    V   22   \n",
       "13647312  2016-06-28   1170547            N               36    H   22   \n",
       "13647313  2016-06-28   1170548            N               36    H   22   \n",
       "...              ...       ...          ...              ...  ...  ...   \n",
       "14576919  2016-06-28    660237            N               36    V   55   \n",
       "14576920  2016-06-28    660238            N               36    V   30   \n",
       "14576921  2016-06-28    660240            N               36    V   52   \n",
       "14576922  2016-06-28    660243            N               36    V   32   \n",
       "14576923  2016-06-28    660248            N               36    V   92   \n",
       "\n",
       "          fecha_alta  ind_nuevo  antiguedad  indrel  ... segmento_top  \\\n",
       "13647309  1995-01-16        0.0         257     1.0  ...            1   \n",
       "13647310  2013-08-28        0.0          35     1.0  ...            0   \n",
       "13647311  2013-08-28        0.0          35     1.0  ...            0   \n",
       "13647312  2013-08-28        0.0          35     1.0  ...            0   \n",
       "13647313  2013-08-28        0.0          35     1.0  ...            0   \n",
       "...              ...        ...         ...     ...  ...          ...   \n",
       "14576919  1999-04-21        0.0         207     1.0  ...            1   \n",
       "14576920  2006-11-29        0.0         116     1.0  ...            0   \n",
       "14576921  2006-11-29        0.0         116     1.0  ...            0   \n",
       "14576922  2006-11-29        0.0         116     1.0  ...            0   \n",
       "14576923  2006-11-29        0.0         116     1.0  ...            0   \n",
       "\n",
       "          segmento_particulares segmento_universitario indfall_s  \\\n",
       "13647309                      0                      0         0   \n",
       "13647310                      1                      0         0   \n",
       "13647311                      0                      1         0   \n",
       "13647312                      0                      1         0   \n",
       "13647313                      0                      1         0   \n",
       "...                         ...                    ...       ...   \n",
       "14576919                      0                      0         0   \n",
       "14576920                      1                      0         0   \n",
       "14576921                      1                      0         0   \n",
       "14576922                      1                      0         0   \n",
       "14576923                      1                      0         0   \n",
       "\n",
       "         tiprel_1mes_a tiprel_1mes_i  tiprel_1mes_p tiprel_1mes_r  indrel_1  \\\n",
       "13647309             1             0              0             0         1   \n",
       "13647310             0             1              0             0         1   \n",
       "13647311             1             0              0             0         1   \n",
       "13647312             0             1              0             0         1   \n",
       "13647313             0             1              0             0         1   \n",
       "...                ...           ...            ...           ...       ...   \n",
       "14576919             1             0              0             0         1   \n",
       "14576920             0             1              0             0         1   \n",
       "14576921             1             0              0             0         1   \n",
       "14576922             0             1              0             0         1   \n",
       "14576923             1             0              0             0         1   \n",
       "\n",
       "          indrel_99  \n",
       "13647309          0  \n",
       "13647310          0  \n",
       "13647311          0  \n",
       "13647312          0  \n",
       "13647313          0  \n",
       "...             ...  \n",
       "14576919          0  \n",
       "14576920          0  \n",
       "14576921          0  \n",
       "14576922          0  \n",
       "14576923          0  \n",
       "\n",
       "[929615 rows x 77 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"fecha_dato\"]== '2016-06-28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ab9495f-8473-4c17-99f4-b7a4dabf7855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:1: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join train with prev1...\n",
      "join train with prev1: cpu 2.31, time 2.31\n",
      "\n",
      "join train with prev2...\n",
      "join train with prev2: cpu 2.30, time 2.30\n",
      "\n",
      "join train with prev3...\n",
      "join train with prev3: cpu 2.41, time 2.41\n",
      "\n",
      "join train with prev4...\n",
      "join train with prev4: cpu 2.61, time 2.61\n",
      "\n",
      "join train with prev5...\n",
      "join train with prev5: cpu 2.66, time 2.66\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:27: RuntimeWarning: All-NaN slice encountered\n",
      "/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:30: RuntimeWarning: All-NaN slice encountered\n"
     ]
    }
   ],
   "source": [
    "train_df, prev_dfs, features, prod_features = load_data()\n",
    "\n",
    "for i, prev_df in enumerate(prev_dfs):\n",
    "    with Timer(\"join train with prev%s\" % (i+1)):\n",
    "        # how = \"inner\" if i == 0 else \"left\"\n",
    "        how = \"left\"\n",
    "        train_df = join_with_prev(train_df, prev_df, how=how)\n",
    "        \n",
    "        ## 結合後0うめ行うロジック追加\n",
    "        ## or テスト側だけロジック変更する → なし。テストユーザの金融商品は基本0\n",
    "\n",
    "# 24個の金融変数に対して for loopをまわします。\n",
    "for prod in products:\n",
    "    # [1~3], [1~5], [2~5] の3つの区間に対して標準偏差を求めます。\n",
    "    for begin, end in [(1,3),(1,5),(2,5)]:\n",
    "        prods = [\"%s_prev%s\" % (prod, i) for i in range(begin,end+1)]\n",
    "        mp_df = train_df[prods].values\n",
    "        stdf = \"%s_std_%s_%s\" % (prod,begin,end)\n",
    "\n",
    "        # np.nanstdで標準偏差を求め、featuresに新規派生変数の名前を追加します。\n",
    "        train_df[stdf] = np.nanstd(mp_df, axis=1)\n",
    "        features += (stdf,)\n",
    "\n",
    "    # [2~3], [2~5] の2つの区間に対して最小値／最大値を求めます。\n",
    "    for begin, end in [(2,3),(2,5)]:\n",
    "        prods = [\"%s_prev%s\" % (prod, i) for i in range(begin,end+1)]\n",
    "        mp_df = train_df[prods].values\n",
    "\n",
    "        minf = \"%s_min_%s_%s\"%(prod,begin,end)\n",
    "        train_df[minf] = np.nanmin(mp_df, axis=1).astype(np.int8)\n",
    "\n",
    "        maxf = \"%s_max_%s_%s\"%(prod,begin,end)\n",
    "        train_df[maxf] = np.nanmax(mp_df, axis=1).astype(np.int8)\n",
    "\n",
    "        features += (minf,maxf,)\n",
    "\n",
    "# 顧客の固有識別番号(ncodpers), 整数で表現された日付(int_date), 実際の日付(fecha_dato), 24個の金融変数(products)と学習に使用するために前処理／特徴量エンジニアリングをした変数(features)が重要な変数です。\n",
    "leave_columns = [\"ncodpers\", \"int_date\", \"fecha_dato\"] + list(products) + list(features)\n",
    "# 重複値がないかを確認します。\n",
    "assert len(leave_columns) == len(set(leave_columns))\n",
    "# train_dfで主要な変数だけを抽出します。\n",
    "train_df = train_df[leave_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57c98a67-79aa-418c-a3f6-d71571388bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ncodpers', 'int_date', 'ind_ahor_fin_ult1_prev5',\n",
       "       'ind_aval_fin_ult1_prev5', 'ind_cco_fin_ult1_prev5',\n",
       "       'ind_cder_fin_ult1_prev5', 'ind_cno_fin_ult1_prev5',\n",
       "       'ind_ctju_fin_ult1_prev5', 'ind_ctma_fin_ult1_prev5',\n",
       "       'ind_ctop_fin_ult1_prev5', 'ind_ctpp_fin_ult1_prev5',\n",
       "       'ind_deco_fin_ult1_prev5', 'ind_deme_fin_ult1_prev5',\n",
       "       'ind_dela_fin_ult1_prev5', 'ind_ecue_fin_ult1_prev5',\n",
       "       'ind_fond_fin_ult1_prev5', 'ind_hip_fin_ult1_prev5',\n",
       "       'ind_plan_fin_ult1_prev5', 'ind_pres_fin_ult1_prev5',\n",
       "       'ind_reca_fin_ult1_prev5', 'ind_tjcr_fin_ult1_prev5',\n",
       "       'ind_valo_fin_ult1_prev5', 'ind_viv_fin_ult1_prev5',\n",
       "       'ind_nomina_ult1_prev5', 'ind_nom_pens_ult1_prev5',\n",
       "       'ind_recibo_ult1_prev5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "103a6c6d-bd97-42a6-bb44-0d9205a64c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(929615, 278)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"fecha_dato\"]== '2016-06-28'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd837b1-0192-4fbe-84d2-1ad58a76b101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7344, 278)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4ac3f4-5736-4aa5-b75b-4eca57017c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:180: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join train with prev1...\n",
      "join train with prev1: cpu 0.92, time 0.92\n",
      "\n",
      "join train with prev2...\n",
      "join train with prev2: cpu 0.49, time 0.49\n",
      "\n",
      "join train with prev3...\n",
      "join train with prev3: cpu 0.46, time 0.46\n",
      "\n",
      "join train with prev4...\n",
      "join train with prev4: cpu 0.50, time 0.50\n",
      "\n",
      "join train with prev5...\n",
      "join train with prev5: cpu 0.48, time 0.48\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1671: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n",
      "/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:205: RuntimeWarning: All-NaN slice encountered\n",
      "/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:208: RuntimeWarning: All-NaN slice encountered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max MAP@7 2016-05-28 0.02040816326530612 0.9999999999999999\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 37\n",
      "[LightGBM] [Info] Number of data points in the train set: 22, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -2.947565\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -2.947565\n",
      "[LightGBM] [Info] Start training from score -2.947565\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -2.947565\n",
      "[LightGBM] [Info] Start training from score -2.254418\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -2.100406\n",
      "[LightGBM] [Info] Start training from score -2.401481\n",
      "[LightGBM] [Info] Start training from score -0.750340\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's multi_logloss: 17.722\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[7]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[8]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[9]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[10]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[11]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[12]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[13]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[14]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[15]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[16]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[17]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[18]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[19]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[20]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[21]\tvalid_0's multi_logloss: 17.722\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_logloss: 17.722\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 60\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.004664\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.671331\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.004664\n",
      "[LightGBM] [Info] Start training from score -3.004664\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.004664\n",
      "[LightGBM] [Info] Start training from score -2.311517\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -2.157505\n",
      "[LightGBM] [Info] Start training from score -2.157505\n",
      "[LightGBM] [Info] Start training from score -0.807439\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Feature importance by split:\n",
      "('canal_entrada', 0)\n",
      "('pais_residencia', 0)\n",
      "('age', 0)\n",
      "('renta', 0)\n",
      "('renta_top', 0)\n",
      "('antiguedad', 0)\n",
      "('tipodom', 0)\n",
      "('cod_prov', 0)\n",
      "('fecha_dato_month', 0)\n",
      "('fecha_dato_year', 0)\n",
      "('fecha_alta_month', 0)\n",
      "('fecha_alta_year', 0)\n",
      "('dato_minus_alta', 0)\n",
      "('indresi_n', 0)\n",
      "('indext_s', 0)\n",
      "('conyuemp_n', 0)\n",
      "('sexo_h', 0)\n",
      "('sexo_v', 0)\n",
      "('ind_empleado_a', 0)\n",
      "('ind_empleado_b', 0)\n",
      "('ind_empleado_f', 0)\n",
      "('ind_empleado_n', 0)\n",
      "('ind_nuevo_new', 0)\n",
      "('segmento_top', 0)\n",
      "('segmento_particulares', 0)\n",
      "('segmento_universitario', 0)\n",
      "('indfall_s', 0)\n",
      "('tiprel_1mes_a', 0)\n",
      "('tiprel_1mes_i', 0)\n",
      "('tiprel_1mes_p', 0)\n",
      "('tiprel_1mes_r', 0)\n",
      "('indrel_1', 0)\n",
      "('indrel_99', 0)\n",
      "('ind_actividad_cliente', 0)\n",
      "('indrel_1mes', 0)\n",
      "('ind_ahor_fin_ult1_prev1', 0)\n",
      "('ind_aval_fin_ult1_prev1', 0)\n",
      "('ind_cco_fin_ult1_prev1', 0)\n",
      "('ind_cder_fin_ult1_prev1', 0)\n",
      "('ind_cno_fin_ult1_prev1', 0)\n",
      "('ind_ctju_fin_ult1_prev1', 0)\n",
      "('ind_ctma_fin_ult1_prev1', 0)\n",
      "('ind_ctop_fin_ult1_prev1', 0)\n",
      "('ind_ctpp_fin_ult1_prev1', 0)\n",
      "('ind_deco_fin_ult1_prev1', 0)\n",
      "('ind_deme_fin_ult1_prev1', 0)\n",
      "('ind_dela_fin_ult1_prev1', 0)\n",
      "('ind_ecue_fin_ult1_prev1', 0)\n",
      "('ind_fond_fin_ult1_prev1', 0)\n",
      "('ind_hip_fin_ult1_prev1', 0)\n",
      "('ind_plan_fin_ult1_prev1', 0)\n",
      "('ind_pres_fin_ult1_prev1', 0)\n",
      "('ind_reca_fin_ult1_prev1', 0)\n",
      "('ind_tjcr_fin_ult1_prev1', 0)\n",
      "('ind_valo_fin_ult1_prev1', 0)\n",
      "('ind_viv_fin_ult1_prev1', 0)\n",
      "('ind_nomina_ult1_prev1', 0)\n",
      "('ind_nom_pens_ult1_prev1', 0)\n",
      "('ind_recibo_ult1_prev1', 0)\n",
      "('ind_ahor_fin_ult1_prev2', 0)\n",
      "('ind_aval_fin_ult1_prev2', 0)\n",
      "('ind_cco_fin_ult1_prev2', 0)\n",
      "('ind_cder_fin_ult1_prev2', 0)\n",
      "('ind_cno_fin_ult1_prev2', 0)\n",
      "('ind_ctju_fin_ult1_prev2', 0)\n",
      "('ind_ctma_fin_ult1_prev2', 0)\n",
      "('ind_ctop_fin_ult1_prev2', 0)\n",
      "('ind_ctpp_fin_ult1_prev2', 0)\n",
      "('ind_deco_fin_ult1_prev2', 0)\n",
      "('ind_deme_fin_ult1_prev2', 0)\n",
      "('ind_dela_fin_ult1_prev2', 0)\n",
      "('ind_ecue_fin_ult1_prev2', 0)\n",
      "('ind_fond_fin_ult1_prev2', 0)\n",
      "('ind_hip_fin_ult1_prev2', 0)\n",
      "('ind_plan_fin_ult1_prev2', 0)\n",
      "('ind_pres_fin_ult1_prev2', 0)\n",
      "('ind_reca_fin_ult1_prev2', 0)\n",
      "('ind_tjcr_fin_ult1_prev2', 0)\n",
      "('ind_valo_fin_ult1_prev2', 0)\n",
      "('ind_viv_fin_ult1_prev2', 0)\n",
      "('ind_nomina_ult1_prev2', 0)\n",
      "('ind_nom_pens_ult1_prev2', 0)\n",
      "('ind_recibo_ult1_prev2', 0)\n",
      "('ind_ahor_fin_ult1_std_1_3', 0)\n",
      "('ind_ahor_fin_ult1_std_1_5', 0)\n",
      "('ind_ahor_fin_ult1_std_2_5', 0)\n",
      "('ind_ahor_fin_ult1_min_2_3', 0)\n",
      "('ind_ahor_fin_ult1_max_2_3', 0)\n",
      "('ind_ahor_fin_ult1_min_2_5', 0)\n",
      "('ind_ahor_fin_ult1_max_2_5', 0)\n",
      "('ind_aval_fin_ult1_std_1_3', 0)\n",
      "('ind_aval_fin_ult1_std_1_5', 0)\n",
      "('ind_aval_fin_ult1_std_2_5', 0)\n",
      "('ind_aval_fin_ult1_min_2_3', 0)\n",
      "('ind_aval_fin_ult1_max_2_3', 0)\n",
      "('ind_aval_fin_ult1_min_2_5', 0)\n",
      "('ind_aval_fin_ult1_max_2_5', 0)\n",
      "('ind_cco_fin_ult1_std_1_3', 0)\n",
      "('ind_cco_fin_ult1_std_1_5', 0)\n",
      "('ind_cco_fin_ult1_std_2_5', 0)\n",
      "('ind_cco_fin_ult1_min_2_3', 0)\n",
      "('ind_cco_fin_ult1_max_2_3', 0)\n",
      "('ind_cco_fin_ult1_min_2_5', 0)\n",
      "('ind_cco_fin_ult1_max_2_5', 0)\n",
      "('ind_cder_fin_ult1_std_1_3', 0)\n",
      "('ind_cder_fin_ult1_std_1_5', 0)\n",
      "('ind_cder_fin_ult1_std_2_5', 0)\n",
      "('ind_cder_fin_ult1_min_2_3', 0)\n",
      "('ind_cder_fin_ult1_max_2_3', 0)\n",
      "('ind_cder_fin_ult1_min_2_5', 0)\n",
      "('ind_cder_fin_ult1_max_2_5', 0)\n",
      "('ind_cno_fin_ult1_std_1_3', 0)\n",
      "('ind_cno_fin_ult1_std_1_5', 0)\n",
      "('ind_cno_fin_ult1_std_2_5', 0)\n",
      "('ind_cno_fin_ult1_min_2_3', 0)\n",
      "('ind_cno_fin_ult1_max_2_3', 0)\n",
      "('ind_cno_fin_ult1_min_2_5', 0)\n",
      "('ind_cno_fin_ult1_max_2_5', 0)\n",
      "('ind_ctju_fin_ult1_std_1_3', 0)\n",
      "('ind_ctju_fin_ult1_std_1_5', 0)\n",
      "('ind_ctju_fin_ult1_std_2_5', 0)\n",
      "('ind_ctju_fin_ult1_min_2_3', 0)\n",
      "('ind_ctju_fin_ult1_max_2_3', 0)\n",
      "('ind_ctju_fin_ult1_min_2_5', 0)\n",
      "('ind_ctju_fin_ult1_max_2_5', 0)\n",
      "('ind_ctma_fin_ult1_std_1_3', 0)\n",
      "('ind_ctma_fin_ult1_std_1_5', 0)\n",
      "('ind_ctma_fin_ult1_std_2_5', 0)\n",
      "('ind_ctma_fin_ult1_min_2_3', 0)\n",
      "('ind_ctma_fin_ult1_max_2_3', 0)\n",
      "('ind_ctma_fin_ult1_min_2_5', 0)\n",
      "('ind_ctma_fin_ult1_max_2_5', 0)\n",
      "('ind_ctop_fin_ult1_std_1_3', 0)\n",
      "('ind_ctop_fin_ult1_std_1_5', 0)\n",
      "('ind_ctop_fin_ult1_std_2_5', 0)\n",
      "('ind_ctop_fin_ult1_min_2_3', 0)\n",
      "('ind_ctop_fin_ult1_max_2_3', 0)\n",
      "('ind_ctop_fin_ult1_min_2_5', 0)\n",
      "('ind_ctop_fin_ult1_max_2_5', 0)\n",
      "('ind_ctpp_fin_ult1_std_1_3', 0)\n",
      "('ind_ctpp_fin_ult1_std_1_5', 0)\n",
      "('ind_ctpp_fin_ult1_std_2_5', 0)\n",
      "('ind_ctpp_fin_ult1_min_2_3', 0)\n",
      "('ind_ctpp_fin_ult1_max_2_3', 0)\n",
      "('ind_ctpp_fin_ult1_min_2_5', 0)\n",
      "('ind_ctpp_fin_ult1_max_2_5', 0)\n",
      "('ind_deco_fin_ult1_std_1_3', 0)\n",
      "('ind_deco_fin_ult1_std_1_5', 0)\n",
      "('ind_deco_fin_ult1_std_2_5', 0)\n",
      "('ind_deco_fin_ult1_min_2_3', 0)\n",
      "('ind_deco_fin_ult1_max_2_3', 0)\n",
      "('ind_deco_fin_ult1_min_2_5', 0)\n",
      "('ind_deco_fin_ult1_max_2_5', 0)\n",
      "('ind_deme_fin_ult1_std_1_3', 0)\n",
      "('ind_deme_fin_ult1_std_1_5', 0)\n",
      "('ind_deme_fin_ult1_std_2_5', 0)\n",
      "('ind_deme_fin_ult1_min_2_3', 0)\n",
      "('ind_deme_fin_ult1_max_2_3', 0)\n",
      "('ind_deme_fin_ult1_min_2_5', 0)\n",
      "('ind_deme_fin_ult1_max_2_5', 0)\n",
      "('ind_dela_fin_ult1_std_1_3', 0)\n",
      "('ind_dela_fin_ult1_std_1_5', 0)\n",
      "('ind_dela_fin_ult1_std_2_5', 0)\n",
      "('ind_dela_fin_ult1_min_2_3', 0)\n",
      "('ind_dela_fin_ult1_max_2_3', 0)\n",
      "('ind_dela_fin_ult1_min_2_5', 0)\n",
      "('ind_dela_fin_ult1_max_2_5', 0)\n",
      "('ind_ecue_fin_ult1_std_1_3', 0)\n",
      "('ind_ecue_fin_ult1_std_1_5', 0)\n",
      "('ind_ecue_fin_ult1_std_2_5', 0)\n",
      "('ind_ecue_fin_ult1_min_2_3', 0)\n",
      "('ind_ecue_fin_ult1_max_2_3', 0)\n",
      "('ind_ecue_fin_ult1_min_2_5', 0)\n",
      "('ind_ecue_fin_ult1_max_2_5', 0)\n",
      "('ind_fond_fin_ult1_std_1_3', 0)\n",
      "('ind_fond_fin_ult1_std_1_5', 0)\n",
      "('ind_fond_fin_ult1_std_2_5', 0)\n",
      "('ind_fond_fin_ult1_min_2_3', 0)\n",
      "('ind_fond_fin_ult1_max_2_3', 0)\n",
      "('ind_fond_fin_ult1_min_2_5', 0)\n",
      "('ind_fond_fin_ult1_max_2_5', 0)\n",
      "('ind_hip_fin_ult1_std_1_3', 0)\n",
      "('ind_hip_fin_ult1_std_1_5', 0)\n",
      "('ind_hip_fin_ult1_std_2_5', 0)\n",
      "('ind_hip_fin_ult1_min_2_3', 0)\n",
      "('ind_hip_fin_ult1_max_2_3', 0)\n",
      "('ind_hip_fin_ult1_min_2_5', 0)\n",
      "('ind_hip_fin_ult1_max_2_5', 0)\n",
      "('ind_plan_fin_ult1_std_1_3', 0)\n",
      "('ind_plan_fin_ult1_std_1_5', 0)\n",
      "('ind_plan_fin_ult1_std_2_5', 0)\n",
      "('ind_plan_fin_ult1_min_2_3', 0)\n",
      "('ind_plan_fin_ult1_max_2_3', 0)\n",
      "('ind_plan_fin_ult1_min_2_5', 0)\n",
      "('ind_plan_fin_ult1_max_2_5', 0)\n",
      "('ind_pres_fin_ult1_std_1_3', 0)\n",
      "('ind_pres_fin_ult1_std_1_5', 0)\n",
      "('ind_pres_fin_ult1_std_2_5', 0)\n",
      "('ind_pres_fin_ult1_min_2_3', 0)\n",
      "('ind_pres_fin_ult1_max_2_3', 0)\n",
      "('ind_pres_fin_ult1_min_2_5', 0)\n",
      "('ind_pres_fin_ult1_max_2_5', 0)\n",
      "('ind_reca_fin_ult1_std_1_3', 0)\n",
      "('ind_reca_fin_ult1_std_1_5', 0)\n",
      "('ind_reca_fin_ult1_std_2_5', 0)\n",
      "('ind_reca_fin_ult1_min_2_3', 0)\n",
      "('ind_reca_fin_ult1_max_2_3', 0)\n",
      "('ind_reca_fin_ult1_min_2_5', 0)\n",
      "('ind_reca_fin_ult1_max_2_5', 0)\n",
      "('ind_tjcr_fin_ult1_std_1_3', 0)\n",
      "('ind_tjcr_fin_ult1_std_1_5', 0)\n",
      "('ind_tjcr_fin_ult1_std_2_5', 0)\n",
      "('ind_tjcr_fin_ult1_min_2_3', 0)\n",
      "('ind_tjcr_fin_ult1_max_2_3', 0)\n",
      "('ind_tjcr_fin_ult1_min_2_5', 0)\n",
      "('ind_tjcr_fin_ult1_max_2_5', 0)\n",
      "('ind_valo_fin_ult1_std_1_3', 0)\n",
      "('ind_valo_fin_ult1_std_1_5', 0)\n",
      "('ind_valo_fin_ult1_std_2_5', 0)\n",
      "('ind_valo_fin_ult1_min_2_3', 0)\n",
      "('ind_valo_fin_ult1_max_2_3', 0)\n",
      "('ind_valo_fin_ult1_min_2_5', 0)\n",
      "('ind_valo_fin_ult1_max_2_5', 0)\n",
      "('ind_viv_fin_ult1_std_1_3', 0)\n",
      "('ind_viv_fin_ult1_std_1_5', 0)\n",
      "('ind_viv_fin_ult1_std_2_5', 0)\n",
      "('ind_viv_fin_ult1_min_2_3', 0)\n",
      "('ind_viv_fin_ult1_max_2_3', 0)\n",
      "('ind_viv_fin_ult1_min_2_5', 0)\n",
      "('ind_viv_fin_ult1_max_2_5', 0)\n",
      "('ind_nomina_ult1_std_1_3', 0)\n",
      "('ind_nomina_ult1_std_1_5', 0)\n",
      "('ind_nomina_ult1_std_2_5', 0)\n",
      "('ind_nomina_ult1_min_2_3', 0)\n",
      "('ind_nomina_ult1_max_2_3', 0)\n",
      "('ind_nomina_ult1_min_2_5', 0)\n",
      "('ind_nomina_ult1_max_2_5', 0)\n",
      "('ind_nom_pens_ult1_std_1_3', 0)\n",
      "('ind_nom_pens_ult1_std_1_5', 0)\n",
      "('ind_nom_pens_ult1_std_2_5', 0)\n",
      "('ind_nom_pens_ult1_min_2_3', 0)\n",
      "('ind_nom_pens_ult1_max_2_3', 0)\n",
      "('ind_nom_pens_ult1_min_2_5', 0)\n",
      "('ind_nom_pens_ult1_max_2_5', 0)\n",
      "('ind_recibo_ult1_std_1_3', 0)\n",
      "('ind_recibo_ult1_std_1_5', 0)\n",
      "('ind_recibo_ult1_std_2_5', 0)\n",
      "('ind_recibo_ult1_min_2_3', 0)\n",
      "('ind_recibo_ult1_max_2_3', 0)\n",
      "('ind_recibo_ult1_min_2_5', 0)\n",
      "('ind_recibo_ult1_max_2_5', 0)\n",
      "Feature importance by gain:\n",
      "('canal_entrada', 0.0)\n",
      "('pais_residencia', 0.0)\n",
      "('age', 0.0)\n",
      "('renta', 0.0)\n",
      "('renta_top', 0.0)\n",
      "('antiguedad', 0.0)\n",
      "('tipodom', 0.0)\n",
      "('cod_prov', 0.0)\n",
      "('fecha_dato_month', 0.0)\n",
      "('fecha_dato_year', 0.0)\n",
      "('fecha_alta_month', 0.0)\n",
      "('fecha_alta_year', 0.0)\n",
      "('dato_minus_alta', 0.0)\n",
      "('indresi_n', 0.0)\n",
      "('indext_s', 0.0)\n",
      "('conyuemp_n', 0.0)\n",
      "('sexo_h', 0.0)\n",
      "('sexo_v', 0.0)\n",
      "('ind_empleado_a', 0.0)\n",
      "('ind_empleado_b', 0.0)\n",
      "('ind_empleado_f', 0.0)\n",
      "('ind_empleado_n', 0.0)\n",
      "('ind_nuevo_new', 0.0)\n",
      "('segmento_top', 0.0)\n",
      "('segmento_particulares', 0.0)\n",
      "('segmento_universitario', 0.0)\n",
      "('indfall_s', 0.0)\n",
      "('tiprel_1mes_a', 0.0)\n",
      "('tiprel_1mes_i', 0.0)\n",
      "('tiprel_1mes_p', 0.0)\n",
      "('tiprel_1mes_r', 0.0)\n",
      "('indrel_1', 0.0)\n",
      "('indrel_99', 0.0)\n",
      "('ind_actividad_cliente', 0.0)\n",
      "('indrel_1mes', 0.0)\n",
      "('ind_ahor_fin_ult1_prev1', 0.0)\n",
      "('ind_aval_fin_ult1_prev1', 0.0)\n",
      "('ind_cco_fin_ult1_prev1', 0.0)\n",
      "('ind_cder_fin_ult1_prev1', 0.0)\n",
      "('ind_cno_fin_ult1_prev1', 0.0)\n",
      "('ind_ctju_fin_ult1_prev1', 0.0)\n",
      "('ind_ctma_fin_ult1_prev1', 0.0)\n",
      "('ind_ctop_fin_ult1_prev1', 0.0)\n",
      "('ind_ctpp_fin_ult1_prev1', 0.0)\n",
      "('ind_deco_fin_ult1_prev1', 0.0)\n",
      "('ind_deme_fin_ult1_prev1', 0.0)\n",
      "('ind_dela_fin_ult1_prev1', 0.0)\n",
      "('ind_ecue_fin_ult1_prev1', 0.0)\n",
      "('ind_fond_fin_ult1_prev1', 0.0)\n",
      "('ind_hip_fin_ult1_prev1', 0.0)\n",
      "('ind_plan_fin_ult1_prev1', 0.0)\n",
      "('ind_pres_fin_ult1_prev1', 0.0)\n",
      "('ind_reca_fin_ult1_prev1', 0.0)\n",
      "('ind_tjcr_fin_ult1_prev1', 0.0)\n",
      "('ind_valo_fin_ult1_prev1', 0.0)\n",
      "('ind_viv_fin_ult1_prev1', 0.0)\n",
      "('ind_nomina_ult1_prev1', 0.0)\n",
      "('ind_nom_pens_ult1_prev1', 0.0)\n",
      "('ind_recibo_ult1_prev1', 0.0)\n",
      "('ind_ahor_fin_ult1_prev2', 0.0)\n",
      "('ind_aval_fin_ult1_prev2', 0.0)\n",
      "('ind_cco_fin_ult1_prev2', 0.0)\n",
      "('ind_cder_fin_ult1_prev2', 0.0)\n",
      "('ind_cno_fin_ult1_prev2', 0.0)\n",
      "('ind_ctju_fin_ult1_prev2', 0.0)\n",
      "('ind_ctma_fin_ult1_prev2', 0.0)\n",
      "('ind_ctop_fin_ult1_prev2', 0.0)\n",
      "('ind_ctpp_fin_ult1_prev2', 0.0)\n",
      "('ind_deco_fin_ult1_prev2', 0.0)\n",
      "('ind_deme_fin_ult1_prev2', 0.0)\n",
      "('ind_dela_fin_ult1_prev2', 0.0)\n",
      "('ind_ecue_fin_ult1_prev2', 0.0)\n",
      "('ind_fond_fin_ult1_prev2', 0.0)\n",
      "('ind_hip_fin_ult1_prev2', 0.0)\n",
      "('ind_plan_fin_ult1_prev2', 0.0)\n",
      "('ind_pres_fin_ult1_prev2', 0.0)\n",
      "('ind_reca_fin_ult1_prev2', 0.0)\n",
      "('ind_tjcr_fin_ult1_prev2', 0.0)\n",
      "('ind_valo_fin_ult1_prev2', 0.0)\n",
      "('ind_viv_fin_ult1_prev2', 0.0)\n",
      "('ind_nomina_ult1_prev2', 0.0)\n",
      "('ind_nom_pens_ult1_prev2', 0.0)\n",
      "('ind_recibo_ult1_prev2', 0.0)\n",
      "('ind_ahor_fin_ult1_std_1_3', 0.0)\n",
      "('ind_ahor_fin_ult1_std_1_5', 0.0)\n",
      "('ind_ahor_fin_ult1_std_2_5', 0.0)\n",
      "('ind_ahor_fin_ult1_min_2_3', 0.0)\n",
      "('ind_ahor_fin_ult1_max_2_3', 0.0)\n",
      "('ind_ahor_fin_ult1_min_2_5', 0.0)\n",
      "('ind_ahor_fin_ult1_max_2_5', 0.0)\n",
      "('ind_aval_fin_ult1_std_1_3', 0.0)\n",
      "('ind_aval_fin_ult1_std_1_5', 0.0)\n",
      "('ind_aval_fin_ult1_std_2_5', 0.0)\n",
      "('ind_aval_fin_ult1_min_2_3', 0.0)\n",
      "('ind_aval_fin_ult1_max_2_3', 0.0)\n",
      "('ind_aval_fin_ult1_min_2_5', 0.0)\n",
      "('ind_aval_fin_ult1_max_2_5', 0.0)\n",
      "('ind_cco_fin_ult1_std_1_3', 0.0)\n",
      "('ind_cco_fin_ult1_std_1_5', 0.0)\n",
      "('ind_cco_fin_ult1_std_2_5', 0.0)\n",
      "('ind_cco_fin_ult1_min_2_3', 0.0)\n",
      "('ind_cco_fin_ult1_max_2_3', 0.0)\n",
      "('ind_cco_fin_ult1_min_2_5', 0.0)\n",
      "('ind_cco_fin_ult1_max_2_5', 0.0)\n",
      "('ind_cder_fin_ult1_std_1_3', 0.0)\n",
      "('ind_cder_fin_ult1_std_1_5', 0.0)\n",
      "('ind_cder_fin_ult1_std_2_5', 0.0)\n",
      "('ind_cder_fin_ult1_min_2_3', 0.0)\n",
      "('ind_cder_fin_ult1_max_2_3', 0.0)\n",
      "('ind_cder_fin_ult1_min_2_5', 0.0)\n",
      "('ind_cder_fin_ult1_max_2_5', 0.0)\n",
      "('ind_cno_fin_ult1_std_1_3', 0.0)\n",
      "('ind_cno_fin_ult1_std_1_5', 0.0)\n",
      "('ind_cno_fin_ult1_std_2_5', 0.0)\n",
      "('ind_cno_fin_ult1_min_2_3', 0.0)\n",
      "('ind_cno_fin_ult1_max_2_3', 0.0)\n",
      "('ind_cno_fin_ult1_min_2_5', 0.0)\n",
      "('ind_cno_fin_ult1_max_2_5', 0.0)\n",
      "('ind_ctju_fin_ult1_std_1_3', 0.0)\n",
      "('ind_ctju_fin_ult1_std_1_5', 0.0)\n",
      "('ind_ctju_fin_ult1_std_2_5', 0.0)\n",
      "('ind_ctju_fin_ult1_min_2_3', 0.0)\n",
      "('ind_ctju_fin_ult1_max_2_3', 0.0)\n",
      "('ind_ctju_fin_ult1_min_2_5', 0.0)\n",
      "('ind_ctju_fin_ult1_max_2_5', 0.0)\n",
      "('ind_ctma_fin_ult1_std_1_3', 0.0)\n",
      "('ind_ctma_fin_ult1_std_1_5', 0.0)\n",
      "('ind_ctma_fin_ult1_std_2_5', 0.0)\n",
      "('ind_ctma_fin_ult1_min_2_3', 0.0)\n",
      "('ind_ctma_fin_ult1_max_2_3', 0.0)\n",
      "('ind_ctma_fin_ult1_min_2_5', 0.0)\n",
      "('ind_ctma_fin_ult1_max_2_5', 0.0)\n",
      "('ind_ctop_fin_ult1_std_1_3', 0.0)\n",
      "('ind_ctop_fin_ult1_std_1_5', 0.0)\n",
      "('ind_ctop_fin_ult1_std_2_5', 0.0)\n",
      "('ind_ctop_fin_ult1_min_2_3', 0.0)\n",
      "('ind_ctop_fin_ult1_max_2_3', 0.0)\n",
      "('ind_ctop_fin_ult1_min_2_5', 0.0)\n",
      "('ind_ctop_fin_ult1_max_2_5', 0.0)\n",
      "('ind_ctpp_fin_ult1_std_1_3', 0.0)\n",
      "('ind_ctpp_fin_ult1_std_1_5', 0.0)\n",
      "('ind_ctpp_fin_ult1_std_2_5', 0.0)\n",
      "('ind_ctpp_fin_ult1_min_2_3', 0.0)\n",
      "('ind_ctpp_fin_ult1_max_2_3', 0.0)\n",
      "('ind_ctpp_fin_ult1_min_2_5', 0.0)\n",
      "('ind_ctpp_fin_ult1_max_2_5', 0.0)\n",
      "('ind_deco_fin_ult1_std_1_3', 0.0)\n",
      "('ind_deco_fin_ult1_std_1_5', 0.0)\n",
      "('ind_deco_fin_ult1_std_2_5', 0.0)\n",
      "('ind_deco_fin_ult1_min_2_3', 0.0)\n",
      "('ind_deco_fin_ult1_max_2_3', 0.0)\n",
      "('ind_deco_fin_ult1_min_2_5', 0.0)\n",
      "('ind_deco_fin_ult1_max_2_5', 0.0)\n",
      "('ind_deme_fin_ult1_std_1_3', 0.0)\n",
      "('ind_deme_fin_ult1_std_1_5', 0.0)\n",
      "('ind_deme_fin_ult1_std_2_5', 0.0)\n",
      "('ind_deme_fin_ult1_min_2_3', 0.0)\n",
      "('ind_deme_fin_ult1_max_2_3', 0.0)\n",
      "('ind_deme_fin_ult1_min_2_5', 0.0)\n",
      "('ind_deme_fin_ult1_max_2_5', 0.0)\n",
      "('ind_dela_fin_ult1_std_1_3', 0.0)\n",
      "('ind_dela_fin_ult1_std_1_5', 0.0)\n",
      "('ind_dela_fin_ult1_std_2_5', 0.0)\n",
      "('ind_dela_fin_ult1_min_2_3', 0.0)\n",
      "('ind_dela_fin_ult1_max_2_3', 0.0)\n",
      "('ind_dela_fin_ult1_min_2_5', 0.0)\n",
      "('ind_dela_fin_ult1_max_2_5', 0.0)\n",
      "('ind_ecue_fin_ult1_std_1_3', 0.0)\n",
      "('ind_ecue_fin_ult1_std_1_5', 0.0)\n",
      "('ind_ecue_fin_ult1_std_2_5', 0.0)\n",
      "('ind_ecue_fin_ult1_min_2_3', 0.0)\n",
      "('ind_ecue_fin_ult1_max_2_3', 0.0)\n",
      "('ind_ecue_fin_ult1_min_2_5', 0.0)\n",
      "('ind_ecue_fin_ult1_max_2_5', 0.0)\n",
      "('ind_fond_fin_ult1_std_1_3', 0.0)\n",
      "('ind_fond_fin_ult1_std_1_5', 0.0)\n",
      "('ind_fond_fin_ult1_std_2_5', 0.0)\n",
      "('ind_fond_fin_ult1_min_2_3', 0.0)\n",
      "('ind_fond_fin_ult1_max_2_3', 0.0)\n",
      "('ind_fond_fin_ult1_min_2_5', 0.0)\n",
      "('ind_fond_fin_ult1_max_2_5', 0.0)\n",
      "('ind_hip_fin_ult1_std_1_3', 0.0)\n",
      "('ind_hip_fin_ult1_std_1_5', 0.0)\n",
      "('ind_hip_fin_ult1_std_2_5', 0.0)\n",
      "('ind_hip_fin_ult1_min_2_3', 0.0)\n",
      "('ind_hip_fin_ult1_max_2_3', 0.0)\n",
      "('ind_hip_fin_ult1_min_2_5', 0.0)\n",
      "('ind_hip_fin_ult1_max_2_5', 0.0)\n",
      "('ind_plan_fin_ult1_std_1_3', 0.0)\n",
      "('ind_plan_fin_ult1_std_1_5', 0.0)\n",
      "('ind_plan_fin_ult1_std_2_5', 0.0)\n",
      "('ind_plan_fin_ult1_min_2_3', 0.0)\n",
      "('ind_plan_fin_ult1_max_2_3', 0.0)\n",
      "('ind_plan_fin_ult1_min_2_5', 0.0)\n",
      "('ind_plan_fin_ult1_max_2_5', 0.0)\n",
      "('ind_pres_fin_ult1_std_1_3', 0.0)\n",
      "('ind_pres_fin_ult1_std_1_5', 0.0)\n",
      "('ind_pres_fin_ult1_std_2_5', 0.0)\n",
      "('ind_pres_fin_ult1_min_2_3', 0.0)\n",
      "('ind_pres_fin_ult1_max_2_3', 0.0)\n",
      "('ind_pres_fin_ult1_min_2_5', 0.0)\n",
      "('ind_pres_fin_ult1_max_2_5', 0.0)\n",
      "('ind_reca_fin_ult1_std_1_3', 0.0)\n",
      "('ind_reca_fin_ult1_std_1_5', 0.0)\n",
      "('ind_reca_fin_ult1_std_2_5', 0.0)\n",
      "('ind_reca_fin_ult1_min_2_3', 0.0)\n",
      "('ind_reca_fin_ult1_max_2_3', 0.0)\n",
      "('ind_reca_fin_ult1_min_2_5', 0.0)\n",
      "('ind_reca_fin_ult1_max_2_5', 0.0)\n",
      "('ind_tjcr_fin_ult1_std_1_3', 0.0)\n",
      "('ind_tjcr_fin_ult1_std_1_5', 0.0)\n",
      "('ind_tjcr_fin_ult1_std_2_5', 0.0)\n",
      "('ind_tjcr_fin_ult1_min_2_3', 0.0)\n",
      "('ind_tjcr_fin_ult1_max_2_3', 0.0)\n",
      "('ind_tjcr_fin_ult1_min_2_5', 0.0)\n",
      "('ind_tjcr_fin_ult1_max_2_5', 0.0)\n",
      "('ind_valo_fin_ult1_std_1_3', 0.0)\n",
      "('ind_valo_fin_ult1_std_1_5', 0.0)\n",
      "('ind_valo_fin_ult1_std_2_5', 0.0)\n",
      "('ind_valo_fin_ult1_min_2_3', 0.0)\n",
      "('ind_valo_fin_ult1_max_2_3', 0.0)\n",
      "('ind_valo_fin_ult1_min_2_5', 0.0)\n",
      "('ind_valo_fin_ult1_max_2_5', 0.0)\n",
      "('ind_viv_fin_ult1_std_1_3', 0.0)\n",
      "('ind_viv_fin_ult1_std_1_5', 0.0)\n",
      "('ind_viv_fin_ult1_std_2_5', 0.0)\n",
      "('ind_viv_fin_ult1_min_2_3', 0.0)\n",
      "('ind_viv_fin_ult1_max_2_3', 0.0)\n",
      "('ind_viv_fin_ult1_min_2_5', 0.0)\n",
      "('ind_viv_fin_ult1_max_2_5', 0.0)\n",
      "('ind_nomina_ult1_std_1_3', 0.0)\n",
      "('ind_nomina_ult1_std_1_5', 0.0)\n",
      "('ind_nomina_ult1_std_2_5', 0.0)\n",
      "('ind_nomina_ult1_min_2_3', 0.0)\n",
      "('ind_nomina_ult1_max_2_3', 0.0)\n",
      "('ind_nomina_ult1_min_2_5', 0.0)\n",
      "('ind_nomina_ult1_max_2_5', 0.0)\n",
      "('ind_nom_pens_ult1_std_1_3', 0.0)\n",
      "('ind_nom_pens_ult1_std_1_5', 0.0)\n",
      "('ind_nom_pens_ult1_std_2_5', 0.0)\n",
      "('ind_nom_pens_ult1_min_2_3', 0.0)\n",
      "('ind_nom_pens_ult1_max_2_3', 0.0)\n",
      "('ind_nom_pens_ult1_min_2_5', 0.0)\n",
      "('ind_nom_pens_ult1_max_2_5', 0.0)\n",
      "('ind_recibo_ult1_std_1_3', 0.0)\n",
      "('ind_recibo_ult1_std_1_5', 0.0)\n",
      "('ind_recibo_ult1_std_2_5', 0.0)\n",
      "('ind_recibo_ult1_min_2_3', 0.0)\n",
      "('ind_recibo_ult1_max_2_3', 0.0)\n",
      "('ind_recibo_ult1_min_2_5', 0.0)\n",
      "('ind_recibo_ult1_max_2_5', 0.0)\n",
      "Y_test_lgbm :  (49, 24)\n",
      "LightGBMlib MAP@7 2016-05-28 0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:45:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:2.61657\teval-mlogloss:3.04266\n",
      "[1]\ttrain-mlogloss:2.29019\teval-mlogloss:2.92816\n",
      "[2]\ttrain-mlogloss:2.05258\teval-mlogloss:2.84573\n",
      "[3]\ttrain-mlogloss:1.86377\teval-mlogloss:2.82665\n",
      "[4]\ttrain-mlogloss:1.71045\teval-mlogloss:2.81013\n",
      "[5]\ttrain-mlogloss:1.57296\teval-mlogloss:2.86932\n",
      "[6]\ttrain-mlogloss:1.46137\teval-mlogloss:2.93487\n",
      "[7]\ttrain-mlogloss:1.36488\teval-mlogloss:2.91273\n",
      "[8]\ttrain-mlogloss:1.26568\teval-mlogloss:2.92937\n",
      "[9]\ttrain-mlogloss:1.19010\teval-mlogloss:2.94544\n",
      "[10]\ttrain-mlogloss:1.11906\teval-mlogloss:3.01006\n",
      "[11]\ttrain-mlogloss:1.05621\teval-mlogloss:3.04066\n",
      "[12]\ttrain-mlogloss:0.99528\teval-mlogloss:3.07262\n",
      "[13]\ttrain-mlogloss:0.94415\teval-mlogloss:3.10916\n",
      "[14]\ttrain-mlogloss:0.89253\teval-mlogloss:3.14503\n",
      "[15]\ttrain-mlogloss:0.84876\teval-mlogloss:3.17397\n",
      "[16]\ttrain-mlogloss:0.80981\teval-mlogloss:3.20400\n",
      "[17]\ttrain-mlogloss:0.77216\teval-mlogloss:3.27441\n",
      "[18]\ttrain-mlogloss:0.73820\teval-mlogloss:3.34755\n",
      "[19]\ttrain-mlogloss:0.70594\teval-mlogloss:3.41822\n",
      "[20]\ttrain-mlogloss:0.67787\teval-mlogloss:3.48224\n",
      "[21]\ttrain-mlogloss:0.66122\teval-mlogloss:3.53757\n",
      "[22]\ttrain-mlogloss:0.64500\teval-mlogloss:3.58697\n",
      "[23]\ttrain-mlogloss:0.62970\teval-mlogloss:3.63335\n",
      "[10:45:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tall_data-mlogloss:3.17805\n",
      "[1]\tall_data-mlogloss:3.17805\n",
      "[2]\tall_data-mlogloss:3.17805\n",
      "[3]\tall_data-mlogloss:3.17805\n",
      "[4]\tall_data-mlogloss:3.17805\n",
      "Feature importance:\n",
      "Y_test_xgb :  (49, 24)\n",
      "XGBoost MAP@7 2016-05-28 0.0 0.0\n",
      "Y_test :  (49, 24)\n",
      "XGBoost+LightGBM MAP@7 2016-05-28 0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/xgboost/core.py:94: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000048 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 25, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -2.359884\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.719697\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.053031\n",
      "[LightGBM] [Info] Start training from score -3.053031\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.053031\n",
      "[LightGBM] [Info] Start training from score -2.359884\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -2.205872\n",
      "[LightGBM] [Info] Start training from score -2.205872\n",
      "[LightGBM] [Info] Start training from score -0.855806\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Feature importance by split:\n",
      "('canal_entrada', 0)\n",
      "('pais_residencia', 0)\n",
      "('age', 0)\n",
      "('renta', 0)\n",
      "('renta_top', 0)\n",
      "('antiguedad', 0)\n",
      "('tipodom', 0)\n",
      "('cod_prov', 0)\n",
      "('fecha_dato_month', 0)\n",
      "('fecha_dato_year', 0)\n",
      "('fecha_alta_month', 0)\n",
      "('fecha_alta_year', 0)\n",
      "('dato_minus_alta', 0)\n",
      "('indresi_n', 0)\n",
      "('indext_s', 0)\n",
      "('conyuemp_n', 0)\n",
      "('sexo_h', 0)\n",
      "('sexo_v', 0)\n",
      "('ind_empleado_a', 0)\n",
      "('ind_empleado_b', 0)\n",
      "('ind_empleado_f', 0)\n",
      "('ind_empleado_n', 0)\n",
      "('ind_nuevo_new', 0)\n",
      "('segmento_top', 0)\n",
      "('segmento_particulares', 0)\n",
      "('segmento_universitario', 0)\n",
      "('indfall_s', 0)\n",
      "('tiprel_1mes_a', 0)\n",
      "('tiprel_1mes_i', 0)\n",
      "('tiprel_1mes_p', 0)\n",
      "('tiprel_1mes_r', 0)\n",
      "('indrel_1', 0)\n",
      "('indrel_99', 0)\n",
      "('ind_actividad_cliente', 0)\n",
      "('indrel_1mes', 0)\n",
      "('ind_ahor_fin_ult1_prev1', 0)\n",
      "('ind_aval_fin_ult1_prev1', 0)\n",
      "('ind_cco_fin_ult1_prev1', 0)\n",
      "('ind_cder_fin_ult1_prev1', 0)\n",
      "('ind_cno_fin_ult1_prev1', 0)\n",
      "('ind_ctju_fin_ult1_prev1', 0)\n",
      "('ind_ctma_fin_ult1_prev1', 0)\n",
      "('ind_ctop_fin_ult1_prev1', 0)\n",
      "('ind_ctpp_fin_ult1_prev1', 0)\n",
      "('ind_deco_fin_ult1_prev1', 0)\n",
      "('ind_deme_fin_ult1_prev1', 0)\n",
      "('ind_dela_fin_ult1_prev1', 0)\n",
      "('ind_ecue_fin_ult1_prev1', 0)\n",
      "('ind_fond_fin_ult1_prev1', 0)\n",
      "('ind_hip_fin_ult1_prev1', 0)\n",
      "('ind_plan_fin_ult1_prev1', 0)\n",
      "('ind_pres_fin_ult1_prev1', 0)\n",
      "('ind_reca_fin_ult1_prev1', 0)\n",
      "('ind_tjcr_fin_ult1_prev1', 0)\n",
      "('ind_valo_fin_ult1_prev1', 0)\n",
      "('ind_viv_fin_ult1_prev1', 0)\n",
      "('ind_nomina_ult1_prev1', 0)\n",
      "('ind_nom_pens_ult1_prev1', 0)\n",
      "('ind_recibo_ult1_prev1', 0)\n",
      "('ind_ahor_fin_ult1_prev2', 0)\n",
      "('ind_aval_fin_ult1_prev2', 0)\n",
      "('ind_cco_fin_ult1_prev2', 0)\n",
      "('ind_cder_fin_ult1_prev2', 0)\n",
      "('ind_cno_fin_ult1_prev2', 0)\n",
      "('ind_ctju_fin_ult1_prev2', 0)\n",
      "('ind_ctma_fin_ult1_prev2', 0)\n",
      "('ind_ctop_fin_ult1_prev2', 0)\n",
      "('ind_ctpp_fin_ult1_prev2', 0)\n",
      "('ind_deco_fin_ult1_prev2', 0)\n",
      "('ind_deme_fin_ult1_prev2', 0)\n",
      "('ind_dela_fin_ult1_prev2', 0)\n",
      "('ind_ecue_fin_ult1_prev2', 0)\n",
      "('ind_fond_fin_ult1_prev2', 0)\n",
      "('ind_hip_fin_ult1_prev2', 0)\n",
      "('ind_plan_fin_ult1_prev2', 0)\n",
      "('ind_pres_fin_ult1_prev2', 0)\n",
      "('ind_reca_fin_ult1_prev2', 0)\n",
      "('ind_tjcr_fin_ult1_prev2', 0)\n",
      "('ind_valo_fin_ult1_prev2', 0)\n",
      "('ind_viv_fin_ult1_prev2', 0)\n",
      "('ind_nomina_ult1_prev2', 0)\n",
      "('ind_nom_pens_ult1_prev2', 0)\n",
      "('ind_recibo_ult1_prev2', 0)\n",
      "('ind_ahor_fin_ult1_std_1_3', 0)\n",
      "('ind_ahor_fin_ult1_std_1_5', 0)\n",
      "('ind_ahor_fin_ult1_std_2_5', 0)\n",
      "('ind_ahor_fin_ult1_min_2_3', 0)\n",
      "('ind_ahor_fin_ult1_max_2_3', 0)\n",
      "('ind_ahor_fin_ult1_min_2_5', 0)\n",
      "('ind_ahor_fin_ult1_max_2_5', 0)\n",
      "('ind_aval_fin_ult1_std_1_3', 0)\n",
      "('ind_aval_fin_ult1_std_1_5', 0)\n",
      "('ind_aval_fin_ult1_std_2_5', 0)\n",
      "('ind_aval_fin_ult1_min_2_3', 0)\n",
      "('ind_aval_fin_ult1_max_2_3', 0)\n",
      "('ind_aval_fin_ult1_min_2_5', 0)\n",
      "('ind_aval_fin_ult1_max_2_5', 0)\n",
      "('ind_cco_fin_ult1_std_1_3', 0)\n",
      "('ind_cco_fin_ult1_std_1_5', 0)\n",
      "('ind_cco_fin_ult1_std_2_5', 0)\n",
      "('ind_cco_fin_ult1_min_2_3', 0)\n",
      "('ind_cco_fin_ult1_max_2_3', 0)\n",
      "('ind_cco_fin_ult1_min_2_5', 0)\n",
      "('ind_cco_fin_ult1_max_2_5', 0)\n",
      "('ind_cder_fin_ult1_std_1_3', 0)\n",
      "('ind_cder_fin_ult1_std_1_5', 0)\n",
      "('ind_cder_fin_ult1_std_2_5', 0)\n",
      "('ind_cder_fin_ult1_min_2_3', 0)\n",
      "('ind_cder_fin_ult1_max_2_3', 0)\n",
      "('ind_cder_fin_ult1_min_2_5', 0)\n",
      "('ind_cder_fin_ult1_max_2_5', 0)\n",
      "('ind_cno_fin_ult1_std_1_3', 0)\n",
      "('ind_cno_fin_ult1_std_1_5', 0)\n",
      "('ind_cno_fin_ult1_std_2_5', 0)\n",
      "('ind_cno_fin_ult1_min_2_3', 0)\n",
      "('ind_cno_fin_ult1_max_2_3', 0)\n",
      "('ind_cno_fin_ult1_min_2_5', 0)\n",
      "('ind_cno_fin_ult1_max_2_5', 0)\n",
      "('ind_ctju_fin_ult1_std_1_3', 0)\n",
      "('ind_ctju_fin_ult1_std_1_5', 0)\n",
      "('ind_ctju_fin_ult1_std_2_5', 0)\n",
      "('ind_ctju_fin_ult1_min_2_3', 0)\n",
      "('ind_ctju_fin_ult1_max_2_3', 0)\n",
      "('ind_ctju_fin_ult1_min_2_5', 0)\n",
      "('ind_ctju_fin_ult1_max_2_5', 0)\n",
      "('ind_ctma_fin_ult1_std_1_3', 0)\n",
      "('ind_ctma_fin_ult1_std_1_5', 0)\n",
      "('ind_ctma_fin_ult1_std_2_5', 0)\n",
      "('ind_ctma_fin_ult1_min_2_3', 0)\n",
      "('ind_ctma_fin_ult1_max_2_3', 0)\n",
      "('ind_ctma_fin_ult1_min_2_5', 0)\n",
      "('ind_ctma_fin_ult1_max_2_5', 0)\n",
      "('ind_ctop_fin_ult1_std_1_3', 0)\n",
      "('ind_ctop_fin_ult1_std_1_5', 0)\n",
      "('ind_ctop_fin_ult1_std_2_5', 0)\n",
      "('ind_ctop_fin_ult1_min_2_3', 0)\n",
      "('ind_ctop_fin_ult1_max_2_3', 0)\n",
      "('ind_ctop_fin_ult1_min_2_5', 0)\n",
      "('ind_ctop_fin_ult1_max_2_5', 0)\n",
      "('ind_ctpp_fin_ult1_std_1_3', 0)\n",
      "('ind_ctpp_fin_ult1_std_1_5', 0)\n",
      "('ind_ctpp_fin_ult1_std_2_5', 0)\n",
      "('ind_ctpp_fin_ult1_min_2_3', 0)\n",
      "('ind_ctpp_fin_ult1_max_2_3', 0)\n",
      "('ind_ctpp_fin_ult1_min_2_5', 0)\n",
      "('ind_ctpp_fin_ult1_max_2_5', 0)\n",
      "('ind_deco_fin_ult1_std_1_3', 0)\n",
      "('ind_deco_fin_ult1_std_1_5', 0)\n",
      "('ind_deco_fin_ult1_std_2_5', 0)\n",
      "('ind_deco_fin_ult1_min_2_3', 0)\n",
      "('ind_deco_fin_ult1_max_2_3', 0)\n",
      "('ind_deco_fin_ult1_min_2_5', 0)\n",
      "('ind_deco_fin_ult1_max_2_5', 0)\n",
      "('ind_deme_fin_ult1_std_1_3', 0)\n",
      "('ind_deme_fin_ult1_std_1_5', 0)\n",
      "('ind_deme_fin_ult1_std_2_5', 0)\n",
      "('ind_deme_fin_ult1_min_2_3', 0)\n",
      "('ind_deme_fin_ult1_max_2_3', 0)\n",
      "('ind_deme_fin_ult1_min_2_5', 0)\n",
      "('ind_deme_fin_ult1_max_2_5', 0)\n",
      "('ind_dela_fin_ult1_std_1_3', 0)\n",
      "('ind_dela_fin_ult1_std_1_5', 0)\n",
      "('ind_dela_fin_ult1_std_2_5', 0)\n",
      "('ind_dela_fin_ult1_min_2_3', 0)\n",
      "('ind_dela_fin_ult1_max_2_3', 0)\n",
      "('ind_dela_fin_ult1_min_2_5', 0)\n",
      "('ind_dela_fin_ult1_max_2_5', 0)\n",
      "('ind_ecue_fin_ult1_std_1_3', 0)\n",
      "('ind_ecue_fin_ult1_std_1_5', 0)\n",
      "('ind_ecue_fin_ult1_std_2_5', 0)\n",
      "('ind_ecue_fin_ult1_min_2_3', 0)\n",
      "('ind_ecue_fin_ult1_max_2_3', 0)\n",
      "('ind_ecue_fin_ult1_min_2_5', 0)\n",
      "('ind_ecue_fin_ult1_max_2_5', 0)\n",
      "('ind_fond_fin_ult1_std_1_3', 0)\n",
      "('ind_fond_fin_ult1_std_1_5', 0)\n",
      "('ind_fond_fin_ult1_std_2_5', 0)\n",
      "('ind_fond_fin_ult1_min_2_3', 0)\n",
      "('ind_fond_fin_ult1_max_2_3', 0)\n",
      "('ind_fond_fin_ult1_min_2_5', 0)\n",
      "('ind_fond_fin_ult1_max_2_5', 0)\n",
      "('ind_hip_fin_ult1_std_1_3', 0)\n",
      "('ind_hip_fin_ult1_std_1_5', 0)\n",
      "('ind_hip_fin_ult1_std_2_5', 0)\n",
      "('ind_hip_fin_ult1_min_2_3', 0)\n",
      "('ind_hip_fin_ult1_max_2_3', 0)\n",
      "('ind_hip_fin_ult1_min_2_5', 0)\n",
      "('ind_hip_fin_ult1_max_2_5', 0)\n",
      "('ind_plan_fin_ult1_std_1_3', 0)\n",
      "('ind_plan_fin_ult1_std_1_5', 0)\n",
      "('ind_plan_fin_ult1_std_2_5', 0)\n",
      "('ind_plan_fin_ult1_min_2_3', 0)\n",
      "('ind_plan_fin_ult1_max_2_3', 0)\n",
      "('ind_plan_fin_ult1_min_2_5', 0)\n",
      "('ind_plan_fin_ult1_max_2_5', 0)\n",
      "('ind_pres_fin_ult1_std_1_3', 0)\n",
      "('ind_pres_fin_ult1_std_1_5', 0)\n",
      "('ind_pres_fin_ult1_std_2_5', 0)\n",
      "('ind_pres_fin_ult1_min_2_3', 0)\n",
      "('ind_pres_fin_ult1_max_2_3', 0)\n",
      "('ind_pres_fin_ult1_min_2_5', 0)\n",
      "('ind_pres_fin_ult1_max_2_5', 0)\n",
      "('ind_reca_fin_ult1_std_1_3', 0)\n",
      "('ind_reca_fin_ult1_std_1_5', 0)\n",
      "('ind_reca_fin_ult1_std_2_5', 0)\n",
      "('ind_reca_fin_ult1_min_2_3', 0)\n",
      "('ind_reca_fin_ult1_max_2_3', 0)\n",
      "('ind_reca_fin_ult1_min_2_5', 0)\n",
      "('ind_reca_fin_ult1_max_2_5', 0)\n",
      "('ind_tjcr_fin_ult1_std_1_3', 0)\n",
      "('ind_tjcr_fin_ult1_std_1_5', 0)\n",
      "('ind_tjcr_fin_ult1_std_2_5', 0)\n",
      "('ind_tjcr_fin_ult1_min_2_3', 0)\n",
      "('ind_tjcr_fin_ult1_max_2_3', 0)\n",
      "('ind_tjcr_fin_ult1_min_2_5', 0)\n",
      "('ind_tjcr_fin_ult1_max_2_5', 0)\n",
      "('ind_valo_fin_ult1_std_1_3', 0)\n",
      "('ind_valo_fin_ult1_std_1_5', 0)\n",
      "('ind_valo_fin_ult1_std_2_5', 0)\n",
      "('ind_valo_fin_ult1_min_2_3', 0)\n",
      "('ind_valo_fin_ult1_max_2_3', 0)\n",
      "('ind_valo_fin_ult1_min_2_5', 0)\n",
      "('ind_valo_fin_ult1_max_2_5', 0)\n",
      "('ind_viv_fin_ult1_std_1_3', 0)\n",
      "('ind_viv_fin_ult1_std_1_5', 0)\n",
      "('ind_viv_fin_ult1_std_2_5', 0)\n",
      "('ind_viv_fin_ult1_min_2_3', 0)\n",
      "('ind_viv_fin_ult1_max_2_3', 0)\n",
      "('ind_viv_fin_ult1_min_2_5', 0)\n",
      "('ind_viv_fin_ult1_max_2_5', 0)\n",
      "('ind_nomina_ult1_std_1_3', 0)\n",
      "('ind_nomina_ult1_std_1_5', 0)\n",
      "('ind_nomina_ult1_std_2_5', 0)\n",
      "('ind_nomina_ult1_min_2_3', 0)\n",
      "('ind_nomina_ult1_max_2_3', 0)\n",
      "('ind_nomina_ult1_min_2_5', 0)\n",
      "('ind_nomina_ult1_max_2_5', 0)\n",
      "('ind_nom_pens_ult1_std_1_3', 0)\n",
      "('ind_nom_pens_ult1_std_1_5', 0)\n",
      "('ind_nom_pens_ult1_std_2_5', 0)\n",
      "('ind_nom_pens_ult1_min_2_3', 0)\n",
      "('ind_nom_pens_ult1_max_2_3', 0)\n",
      "('ind_nom_pens_ult1_min_2_5', 0)\n",
      "('ind_nom_pens_ult1_max_2_5', 0)\n",
      "('ind_recibo_ult1_std_1_3', 0)\n",
      "('ind_recibo_ult1_std_1_5', 0)\n",
      "('ind_recibo_ult1_std_2_5', 0)\n",
      "('ind_recibo_ult1_min_2_3', 0)\n",
      "('ind_recibo_ult1_max_2_3', 0)\n",
      "('ind_recibo_ult1_min_2_5', 0)\n",
      "('ind_recibo_ult1_max_2_5', 0)\n",
      "Feature importance by gain:\n",
      "('canal_entrada', 0.0)\n",
      "('pais_residencia', 0.0)\n",
      "('age', 0.0)\n",
      "('renta', 0.0)\n",
      "('renta_top', 0.0)\n",
      "('antiguedad', 0.0)\n",
      "('tipodom', 0.0)\n",
      "('cod_prov', 0.0)\n",
      "('fecha_dato_month', 0.0)\n",
      "('fecha_dato_year', 0.0)\n",
      "('fecha_alta_month', 0.0)\n",
      "('fecha_alta_year', 0.0)\n",
      "('dato_minus_alta', 0.0)\n",
      "('indresi_n', 0.0)\n",
      "('indext_s', 0.0)\n",
      "('conyuemp_n', 0.0)\n",
      "('sexo_h', 0.0)\n",
      "('sexo_v', 0.0)\n",
      "('ind_empleado_a', 0.0)\n",
      "('ind_empleado_b', 0.0)\n",
      "('ind_empleado_f', 0.0)\n",
      "('ind_empleado_n', 0.0)\n",
      "('ind_nuevo_new', 0.0)\n",
      "('segmento_top', 0.0)\n",
      "('segmento_particulares', 0.0)\n",
      "('segmento_universitario', 0.0)\n",
      "('indfall_s', 0.0)\n",
      "('tiprel_1mes_a', 0.0)\n",
      "('tiprel_1mes_i', 0.0)\n",
      "('tiprel_1mes_p', 0.0)\n",
      "('tiprel_1mes_r', 0.0)\n",
      "('indrel_1', 0.0)\n",
      "('indrel_99', 0.0)\n",
      "('ind_actividad_cliente', 0.0)\n",
      "('indrel_1mes', 0.0)\n",
      "('ind_ahor_fin_ult1_prev1', 0.0)\n",
      "('ind_aval_fin_ult1_prev1', 0.0)\n",
      "('ind_cco_fin_ult1_prev1', 0.0)\n",
      "('ind_cder_fin_ult1_prev1', 0.0)\n",
      "('ind_cno_fin_ult1_prev1', 0.0)\n",
      "('ind_ctju_fin_ult1_prev1', 0.0)\n",
      "('ind_ctma_fin_ult1_prev1', 0.0)\n",
      "('ind_ctop_fin_ult1_prev1', 0.0)\n",
      "('ind_ctpp_fin_ult1_prev1', 0.0)\n",
      "('ind_deco_fin_ult1_prev1', 0.0)\n",
      "('ind_deme_fin_ult1_prev1', 0.0)\n",
      "('ind_dela_fin_ult1_prev1', 0.0)\n",
      "('ind_ecue_fin_ult1_prev1', 0.0)\n",
      "('ind_fond_fin_ult1_prev1', 0.0)\n",
      "('ind_hip_fin_ult1_prev1', 0.0)\n",
      "('ind_plan_fin_ult1_prev1', 0.0)\n",
      "('ind_pres_fin_ult1_prev1', 0.0)\n",
      "('ind_reca_fin_ult1_prev1', 0.0)\n",
      "('ind_tjcr_fin_ult1_prev1', 0.0)\n",
      "('ind_valo_fin_ult1_prev1', 0.0)\n",
      "('ind_viv_fin_ult1_prev1', 0.0)\n",
      "('ind_nomina_ult1_prev1', 0.0)\n",
      "('ind_nom_pens_ult1_prev1', 0.0)\n",
      "('ind_recibo_ult1_prev1', 0.0)\n",
      "('ind_ahor_fin_ult1_prev2', 0.0)\n",
      "('ind_aval_fin_ult1_prev2', 0.0)\n",
      "('ind_cco_fin_ult1_prev2', 0.0)\n",
      "('ind_cder_fin_ult1_prev2', 0.0)\n",
      "('ind_cno_fin_ult1_prev2', 0.0)\n",
      "('ind_ctju_fin_ult1_prev2', 0.0)\n",
      "('ind_ctma_fin_ult1_prev2', 0.0)\n",
      "('ind_ctop_fin_ult1_prev2', 0.0)\n",
      "('ind_ctpp_fin_ult1_prev2', 0.0)\n",
      "('ind_deco_fin_ult1_prev2', 0.0)\n",
      "('ind_deme_fin_ult1_prev2', 0.0)\n",
      "('ind_dela_fin_ult1_prev2', 0.0)\n",
      "('ind_ecue_fin_ult1_prev2', 0.0)\n",
      "('ind_fond_fin_ult1_prev2', 0.0)\n",
      "('ind_hip_fin_ult1_prev2', 0.0)\n",
      "('ind_plan_fin_ult1_prev2', 0.0)\n",
      "('ind_pres_fin_ult1_prev2', 0.0)\n",
      "('ind_reca_fin_ult1_prev2', 0.0)\n",
      "('ind_tjcr_fin_ult1_prev2', 0.0)\n",
      "('ind_valo_fin_ult1_prev2', 0.0)\n",
      "('ind_viv_fin_ult1_prev2', 0.0)\n",
      "('ind_nomina_ult1_prev2', 0.0)\n",
      "('ind_nom_pens_ult1_prev2', 0.0)\n",
      "('ind_recibo_ult1_prev2', 0.0)\n",
      "('ind_ahor_fin_ult1_std_1_3', 0.0)\n",
      "('ind_ahor_fin_ult1_std_1_5', 0.0)\n",
      "('ind_ahor_fin_ult1_std_2_5', 0.0)\n",
      "('ind_ahor_fin_ult1_min_2_3', 0.0)\n",
      "('ind_ahor_fin_ult1_max_2_3', 0.0)\n",
      "('ind_ahor_fin_ult1_min_2_5', 0.0)\n",
      "('ind_ahor_fin_ult1_max_2_5', 0.0)\n",
      "('ind_aval_fin_ult1_std_1_3', 0.0)\n",
      "('ind_aval_fin_ult1_std_1_5', 0.0)\n",
      "('ind_aval_fin_ult1_std_2_5', 0.0)\n",
      "('ind_aval_fin_ult1_min_2_3', 0.0)\n",
      "('ind_aval_fin_ult1_max_2_3', 0.0)\n",
      "('ind_aval_fin_ult1_min_2_5', 0.0)\n",
      "('ind_aval_fin_ult1_max_2_5', 0.0)\n",
      "('ind_cco_fin_ult1_std_1_3', 0.0)\n",
      "('ind_cco_fin_ult1_std_1_5', 0.0)\n",
      "('ind_cco_fin_ult1_std_2_5', 0.0)\n",
      "('ind_cco_fin_ult1_min_2_3', 0.0)\n",
      "('ind_cco_fin_ult1_max_2_3', 0.0)\n",
      "('ind_cco_fin_ult1_min_2_5', 0.0)\n",
      "('ind_cco_fin_ult1_max_2_5', 0.0)\n",
      "('ind_cder_fin_ult1_std_1_3', 0.0)\n",
      "('ind_cder_fin_ult1_std_1_5', 0.0)\n",
      "('ind_cder_fin_ult1_std_2_5', 0.0)\n",
      "('ind_cder_fin_ult1_min_2_3', 0.0)\n",
      "('ind_cder_fin_ult1_max_2_3', 0.0)\n",
      "('ind_cder_fin_ult1_min_2_5', 0.0)\n",
      "('ind_cder_fin_ult1_max_2_5', 0.0)\n",
      "('ind_cno_fin_ult1_std_1_3', 0.0)\n",
      "('ind_cno_fin_ult1_std_1_5', 0.0)\n",
      "('ind_cno_fin_ult1_std_2_5', 0.0)\n",
      "('ind_cno_fin_ult1_min_2_3', 0.0)\n",
      "('ind_cno_fin_ult1_max_2_3', 0.0)\n",
      "('ind_cno_fin_ult1_min_2_5', 0.0)\n",
      "('ind_cno_fin_ult1_max_2_5', 0.0)\n",
      "('ind_ctju_fin_ult1_std_1_3', 0.0)\n",
      "('ind_ctju_fin_ult1_std_1_5', 0.0)\n",
      "('ind_ctju_fin_ult1_std_2_5', 0.0)\n",
      "('ind_ctju_fin_ult1_min_2_3', 0.0)\n",
      "('ind_ctju_fin_ult1_max_2_3', 0.0)\n",
      "('ind_ctju_fin_ult1_min_2_5', 0.0)\n",
      "('ind_ctju_fin_ult1_max_2_5', 0.0)\n",
      "('ind_ctma_fin_ult1_std_1_3', 0.0)\n",
      "('ind_ctma_fin_ult1_std_1_5', 0.0)\n",
      "('ind_ctma_fin_ult1_std_2_5', 0.0)\n",
      "('ind_ctma_fin_ult1_min_2_3', 0.0)\n",
      "('ind_ctma_fin_ult1_max_2_3', 0.0)\n",
      "('ind_ctma_fin_ult1_min_2_5', 0.0)\n",
      "('ind_ctma_fin_ult1_max_2_5', 0.0)\n",
      "('ind_ctop_fin_ult1_std_1_3', 0.0)\n",
      "('ind_ctop_fin_ult1_std_1_5', 0.0)\n",
      "('ind_ctop_fin_ult1_std_2_5', 0.0)\n",
      "('ind_ctop_fin_ult1_min_2_3', 0.0)\n",
      "('ind_ctop_fin_ult1_max_2_3', 0.0)\n",
      "('ind_ctop_fin_ult1_min_2_5', 0.0)\n",
      "('ind_ctop_fin_ult1_max_2_5', 0.0)\n",
      "('ind_ctpp_fin_ult1_std_1_3', 0.0)\n",
      "('ind_ctpp_fin_ult1_std_1_5', 0.0)\n",
      "('ind_ctpp_fin_ult1_std_2_5', 0.0)\n",
      "('ind_ctpp_fin_ult1_min_2_3', 0.0)\n",
      "('ind_ctpp_fin_ult1_max_2_3', 0.0)\n",
      "('ind_ctpp_fin_ult1_min_2_5', 0.0)\n",
      "('ind_ctpp_fin_ult1_max_2_5', 0.0)\n",
      "('ind_deco_fin_ult1_std_1_3', 0.0)\n",
      "('ind_deco_fin_ult1_std_1_5', 0.0)\n",
      "('ind_deco_fin_ult1_std_2_5', 0.0)\n",
      "('ind_deco_fin_ult1_min_2_3', 0.0)\n",
      "('ind_deco_fin_ult1_max_2_3', 0.0)\n",
      "('ind_deco_fin_ult1_min_2_5', 0.0)\n",
      "('ind_deco_fin_ult1_max_2_5', 0.0)\n",
      "('ind_deme_fin_ult1_std_1_3', 0.0)\n",
      "('ind_deme_fin_ult1_std_1_5', 0.0)\n",
      "('ind_deme_fin_ult1_std_2_5', 0.0)\n",
      "('ind_deme_fin_ult1_min_2_3', 0.0)\n",
      "('ind_deme_fin_ult1_max_2_3', 0.0)\n",
      "('ind_deme_fin_ult1_min_2_5', 0.0)\n",
      "('ind_deme_fin_ult1_max_2_5', 0.0)\n",
      "('ind_dela_fin_ult1_std_1_3', 0.0)\n",
      "('ind_dela_fin_ult1_std_1_5', 0.0)\n",
      "('ind_dela_fin_ult1_std_2_5', 0.0)\n",
      "('ind_dela_fin_ult1_min_2_3', 0.0)\n",
      "('ind_dela_fin_ult1_max_2_3', 0.0)\n",
      "('ind_dela_fin_ult1_min_2_5', 0.0)\n",
      "('ind_dela_fin_ult1_max_2_5', 0.0)\n",
      "('ind_ecue_fin_ult1_std_1_3', 0.0)\n",
      "('ind_ecue_fin_ult1_std_1_5', 0.0)\n",
      "('ind_ecue_fin_ult1_std_2_5', 0.0)\n",
      "('ind_ecue_fin_ult1_min_2_3', 0.0)\n",
      "('ind_ecue_fin_ult1_max_2_3', 0.0)\n",
      "('ind_ecue_fin_ult1_min_2_5', 0.0)\n",
      "('ind_ecue_fin_ult1_max_2_5', 0.0)\n",
      "('ind_fond_fin_ult1_std_1_3', 0.0)\n",
      "('ind_fond_fin_ult1_std_1_5', 0.0)\n",
      "('ind_fond_fin_ult1_std_2_5', 0.0)\n",
      "('ind_fond_fin_ult1_min_2_3', 0.0)\n",
      "('ind_fond_fin_ult1_max_2_3', 0.0)\n",
      "('ind_fond_fin_ult1_min_2_5', 0.0)\n",
      "('ind_fond_fin_ult1_max_2_5', 0.0)\n",
      "('ind_hip_fin_ult1_std_1_3', 0.0)\n",
      "('ind_hip_fin_ult1_std_1_5', 0.0)\n",
      "('ind_hip_fin_ult1_std_2_5', 0.0)\n",
      "('ind_hip_fin_ult1_min_2_3', 0.0)\n",
      "('ind_hip_fin_ult1_max_2_3', 0.0)\n",
      "('ind_hip_fin_ult1_min_2_5', 0.0)\n",
      "('ind_hip_fin_ult1_max_2_5', 0.0)\n",
      "('ind_plan_fin_ult1_std_1_3', 0.0)\n",
      "('ind_plan_fin_ult1_std_1_5', 0.0)\n",
      "('ind_plan_fin_ult1_std_2_5', 0.0)\n",
      "('ind_plan_fin_ult1_min_2_3', 0.0)\n",
      "('ind_plan_fin_ult1_max_2_3', 0.0)\n",
      "('ind_plan_fin_ult1_min_2_5', 0.0)\n",
      "('ind_plan_fin_ult1_max_2_5', 0.0)\n",
      "('ind_pres_fin_ult1_std_1_3', 0.0)\n",
      "('ind_pres_fin_ult1_std_1_5', 0.0)\n",
      "('ind_pres_fin_ult1_std_2_5', 0.0)\n",
      "('ind_pres_fin_ult1_min_2_3', 0.0)\n",
      "('ind_pres_fin_ult1_max_2_3', 0.0)\n",
      "('ind_pres_fin_ult1_min_2_5', 0.0)\n",
      "('ind_pres_fin_ult1_max_2_5', 0.0)\n",
      "('ind_reca_fin_ult1_std_1_3', 0.0)\n",
      "('ind_reca_fin_ult1_std_1_5', 0.0)\n",
      "('ind_reca_fin_ult1_std_2_5', 0.0)\n",
      "('ind_reca_fin_ult1_min_2_3', 0.0)\n",
      "('ind_reca_fin_ult1_max_2_3', 0.0)\n",
      "('ind_reca_fin_ult1_min_2_5', 0.0)\n",
      "('ind_reca_fin_ult1_max_2_5', 0.0)\n",
      "('ind_tjcr_fin_ult1_std_1_3', 0.0)\n",
      "('ind_tjcr_fin_ult1_std_1_5', 0.0)\n",
      "('ind_tjcr_fin_ult1_std_2_5', 0.0)\n",
      "('ind_tjcr_fin_ult1_min_2_3', 0.0)\n",
      "('ind_tjcr_fin_ult1_max_2_3', 0.0)\n",
      "('ind_tjcr_fin_ult1_min_2_5', 0.0)\n",
      "('ind_tjcr_fin_ult1_max_2_5', 0.0)\n",
      "('ind_valo_fin_ult1_std_1_3', 0.0)\n",
      "('ind_valo_fin_ult1_std_1_5', 0.0)\n",
      "('ind_valo_fin_ult1_std_2_5', 0.0)\n",
      "('ind_valo_fin_ult1_min_2_3', 0.0)\n",
      "('ind_valo_fin_ult1_max_2_3', 0.0)\n",
      "('ind_valo_fin_ult1_min_2_5', 0.0)\n",
      "('ind_valo_fin_ult1_max_2_5', 0.0)\n",
      "('ind_viv_fin_ult1_std_1_3', 0.0)\n",
      "('ind_viv_fin_ult1_std_1_5', 0.0)\n",
      "('ind_viv_fin_ult1_std_2_5', 0.0)\n",
      "('ind_viv_fin_ult1_min_2_3', 0.0)\n",
      "('ind_viv_fin_ult1_max_2_3', 0.0)\n",
      "('ind_viv_fin_ult1_min_2_5', 0.0)\n",
      "('ind_viv_fin_ult1_max_2_5', 0.0)\n",
      "('ind_nomina_ult1_std_1_3', 0.0)\n",
      "('ind_nomina_ult1_std_1_5', 0.0)\n",
      "('ind_nomina_ult1_std_2_5', 0.0)\n",
      "('ind_nomina_ult1_min_2_3', 0.0)\n",
      "('ind_nomina_ult1_max_2_3', 0.0)\n",
      "('ind_nomina_ult1_min_2_5', 0.0)\n",
      "('ind_nomina_ult1_max_2_5', 0.0)\n",
      "('ind_nom_pens_ult1_std_1_3', 0.0)\n",
      "('ind_nom_pens_ult1_std_1_5', 0.0)\n",
      "('ind_nom_pens_ult1_std_2_5', 0.0)\n",
      "('ind_nom_pens_ult1_min_2_3', 0.0)\n",
      "('ind_nom_pens_ult1_max_2_3', 0.0)\n",
      "('ind_nom_pens_ult1_min_2_5', 0.0)\n",
      "('ind_nom_pens_ult1_max_2_5', 0.0)\n",
      "('ind_recibo_ult1_std_1_3', 0.0)\n",
      "('ind_recibo_ult1_std_1_5', 0.0)\n",
      "('ind_recibo_ult1_std_2_5', 0.0)\n",
      "('ind_recibo_ult1_min_2_3', 0.0)\n",
      "('ind_recibo_ult1_max_2_3', 0.0)\n",
      "('ind_recibo_ult1_min_2_5', 0.0)\n",
      "('ind_recibo_ult1_max_2_5', 0.0)\n",
      "Y_test_lgbm :  (6664, 24)\n",
      "[10:45:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tall_data-mlogloss:3.17805\n",
      "[1]\tall_data-mlogloss:3.17805\n",
      "[2]\tall_data-mlogloss:3.17805\n",
      "[3]\tall_data-mlogloss:3.17805\n",
      "[4]\tall_data-mlogloss:3.17805\n",
      "Feature importance:\n",
      "Y_test_xgb :  (6664, 24)\n",
      "Y_test :  (6664, 24)\n"
     ]
    }
   ],
   "source": [
    "all_df, features, prod_features = make_data()\n",
    "\n",
    "# 特徴量エンジニアリングが完了したデータを保存します。\n",
    "# train_df.to_pickle(\"../data/input/8th.feature_engineer.all.pkl\")\n",
    "all_df.to_pickle(\"../data/input/8th.feature_engineer.all.pkl\")\n",
    "pickle.dump((features, prod_features), open(\"../data/input/8th.feature_engineer.cv_meta.pkl\", \"wb\"))\n",
    "\n",
    "# あとで消す\n",
    "products = (\n",
    "    \"ind_ahor_fin_ult1\",\n",
    "    \"ind_aval_fin_ult1\",\n",
    "    \"ind_cco_fin_ult1\" ,\n",
    "    \"ind_cder_fin_ult1\",\n",
    "    \"ind_cno_fin_ult1\" ,\n",
    "    \"ind_ctju_fin_ult1\",\n",
    "    \"ind_ctma_fin_ult1\",\n",
    "    \"ind_ctop_fin_ult1\",\n",
    "    \"ind_ctpp_fin_ult1\",\n",
    "    \"ind_deco_fin_ult1\",\n",
    "    \"ind_deme_fin_ult1\",\n",
    "    \"ind_dela_fin_ult1\",\n",
    "    \"ind_ecue_fin_ult1\",\n",
    "    \"ind_fond_fin_ult1\",\n",
    "    \"ind_hip_fin_ult1\" ,\n",
    "    \"ind_plan_fin_ult1\",\n",
    "    \"ind_pres_fin_ult1\",\n",
    "    \"ind_reca_fin_ult1\",\n",
    "    \"ind_tjcr_fin_ult1\",\n",
    "    \"ind_valo_fin_ult1\",\n",
    "    \"ind_viv_fin_ult1\" ,\n",
    "    \"ind_nomina_ult1\"  ,\n",
    "    \"ind_nom_pens_ult1\",\n",
    "    \"ind_recibo_ult1\"  ,\n",
    ")\n",
    "\n",
    "\n",
    "# train_predict(all_df, features, prod_features, \"2016-05-28\", cv=True)\n",
    "# train_predict(all_df, features, prod_features, \"2016-06-28\", cv=False)\n",
    "train_predict(all_df, list(features), list(products), \"2016-05-28\", cv=True)\n",
    "train_predict(all_df, list(features), list(products), \"2016-06-28\", cv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78e489-3396-461b-9c54-4dcca5a5d559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
